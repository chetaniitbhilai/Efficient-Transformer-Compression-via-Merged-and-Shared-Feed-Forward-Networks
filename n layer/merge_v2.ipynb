{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca48668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from datasets import load_dataset\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc46d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372b7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44dfa2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ffn_features_single_pass(model, texts, max_tokens=4000):\n",
    "    \"\"\"\n",
    "    Extract FFN pre-activation features (c_fc outputs)\n",
    "    for all layers from a SINGLE forward pass,\n",
    "    excluding padding tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    features = {i: [] for i in range(len(model.transformer.h))}\n",
    "    hooks = []\n",
    "\n",
    "    def make_hook(layer_idx):\n",
    "        def hook(module, inp, out):\n",
    "            # out: [batch, seq, d_ff]\n",
    "            features[layer_idx].append(out.detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    # Register hooks ONLY on c_fc\n",
    "    for i, block in enumerate(model.transformer.h):\n",
    "        hooks.append(\n",
    "            block.mlp.c_fc.register_forward_hook(make_hook(i))\n",
    "        )\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(device)\n",
    "\n",
    "    attention_mask = enc[\"attention_mask\"]  # [B, T]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**enc)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    X = {}\n",
    "    for i in features:\n",
    "        # [B, T, d]\n",
    "        # acts = torch.cat(features[i], dim=0)\n",
    "\n",
    "        # # mask out padding tokens\n",
    "        # mask = attention_mask.bool().unsqueeze(-1)  # [B, T, 1]\n",
    "        # acts = acts[mask.expand_as(acts)].view(-1, acts.shape[-1])\n",
    "\n",
    "        \n",
    "        acts = torch.cat(features[i], dim=0)  # acts is on CPU\n",
    "\n",
    "        mask = attention_mask.bool().unsqueeze(-1).cpu()  # <-- FIX\n",
    "\n",
    "        acts = acts[mask.expand_as(acts)].view(-1, acts.shape[-1])\n",
    "\n",
    "        if acts.shape[0] > max_tokens:\n",
    "            acts = acts[:max_tokens]\n",
    "\n",
    "        X[i] = acts.numpy()\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd3781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "def compute_adjacent_permutations(feature_dict, window_layers):\n",
    "    \"\"\"\n",
    "    Compute permutations ONLY between anchor and adjacent layers\n",
    "    inside a window.\n",
    "    \"\"\"\n",
    "    anchor = window_layers[0]\n",
    "    perms = {}\n",
    "\n",
    "    X_anchor = feature_dict[anchor]\n",
    "\n",
    "    for layer in window_layers[1:]:\n",
    "        X_other = feature_dict[layer]\n",
    "\n",
    "        # Pearson correlation (Eq. 1 in paper)\n",
    "        C = np.corrcoef(X_anchor, X_other, rowvar=False)\n",
    "        d = X_anchor.shape[1]\n",
    "        C = C[:d, d:]\n",
    "\n",
    "        _, col_ind = linear_sum_assignment(-C)\n",
    "        perms[layer] = col_ind\n",
    "\n",
    "    return perms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9600e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(n_layers, k):\n",
    "    \"\"\"\n",
    "    Generate sliding windows of k adjacent layers.\n",
    "    \"\"\"\n",
    "    return [list(range(i, i + k)) for i in range(n_layers - k + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0665f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6904fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_ffn(model, layer, perm):\n",
    "    block = model.transformer.h[layer].mlp\n",
    "    perm = torch.tensor(perm, dtype=torch.long, device=device)\n",
    "\n",
    "    # GPT-2 weight shapes are transposed in your setup:\n",
    "    # c_fc:   [768, 3072]\n",
    "    # c_proj: [3072, 768]\n",
    "    with torch.no_grad():\n",
    "        block.c_fc.weight[:] = block.c_fc.weight[:, perm]\n",
    "        block.c_fc.bias[:]   = block.c_fc.bias[perm]\n",
    "        block.c_proj.weight[:] = block.c_proj.weight[perm, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c9ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def merge_k_layers(model, layers, perms):\n",
    "    \"\"\"\n",
    "    Paper-faithful FFN merging with TRUE weight tying.\n",
    "    - No in-place permutation\n",
    "    - Logical permutation inside averaging\n",
    "    - Shared nn.Parameter objects\n",
    "    \"\"\"\n",
    "\n",
    "    anchor = layers[0]\n",
    "    anchor_mlp = model.transformer.h[anchor].mlp\n",
    "\n",
    "    device = anchor_mlp.c_fc.weight.device\n",
    "    k = len(layers)\n",
    "\n",
    "    # Start from anchor weights\n",
    "    W_in  = anchor_mlp.c_fc.weight.data.clone()\n",
    "    b_in  = anchor_mlp.c_fc.bias.data.clone()\n",
    "    W_out = anchor_mlp.c_proj.weight.data.clone()\n",
    "    b_out = anchor_mlp.c_proj.bias.data.clone()\n",
    "\n",
    "    # Accumulate aligned weights (NO mutation)\n",
    "    for layer in layers[1:]:\n",
    "        mlp = model.transformer.h[layer].mlp\n",
    "        perm = torch.tensor(perms[layer], device=device)\n",
    "\n",
    "        # Logical permutation (as in Eq. 3â€“6)\n",
    "        W_in  += mlp.c_fc.weight[:, perm]\n",
    "        b_in  += mlp.c_fc.bias[perm]\n",
    "        W_out += mlp.c_proj.weight[perm, :]\n",
    "        b_out += mlp.c_proj.bias\n",
    "\n",
    "    # Average\n",
    "    W_in  /= k\n",
    "    b_in  /= k\n",
    "    W_out /= k\n",
    "    b_out /= k\n",
    "\n",
    "    # Create SHARED parameters (true tying)\n",
    "    shared_c_fc_weight   = nn.Parameter(W_in)\n",
    "    shared_c_fc_bias     = nn.Parameter(b_in)\n",
    "    shared_c_proj_weight = nn.Parameter(W_out)\n",
    "    shared_c_proj_bias   = nn.Parameter(b_out)\n",
    "\n",
    "    # Tie all layers in the window\n",
    "    for layer in layers:\n",
    "        mlp = model.transformer.h[layer].mlp\n",
    "\n",
    "        mlp.c_fc.weight = shared_c_fc_weight\n",
    "        mlp.c_fc.bias   = shared_c_fc_bias\n",
    "\n",
    "        mlp.c_proj.weight = shared_c_proj_weight\n",
    "        mlp.c_proj.bias   = shared_c_proj_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0830f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "def compute_perplexity(model, texts, batch_size=4, max_length=128):\n",
    "    model.eval()\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ).to(device)\n",
    "\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, :-1, :]\n",
    "            labels = input_ids[:, 1:]\n",
    "\n",
    "        # mask padding tokens\n",
    "        mask = attention_mask[:, 1:].bool()\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = log_probs.gather(\n",
    "            dim=-1, index=labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        total_nll += -(token_log_probs[mask]).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    return math.exp(total_nll / total_tokens)\n",
    "\n",
    "# # merged_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "# # merge_k_layers(merged_model, window, perms)\n",
    "\n",
    "# merged_ppl = compute_perplexity(model, texts)\n",
    "\n",
    "# print(f\"Merged (window={window}) PPL: {merged_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271ccb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc52aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline GPT-2 PPL: 46.98\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\"\n",
    ")\n",
    "texts = [t for t in dataset[\"text\"] if len(t.strip()) > 0][:200]\n",
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "base_ppl = compute_perplexity(base_model, texts)\n",
    "\n",
    "print(f\"Baseline GPT-2 PPL: {base_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6085f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Before recovery PPL:\",\n",
    "#           compute_perplexity(model, texts))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb754fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transformer layers: 12\n"
     ]
    }
   ],
   "source": [
    "def print_num_layers(model):\n",
    "    num_layers = len(model.transformer.h)\n",
    "    print(f\"Number of transformer layers: {num_layers}\")\n",
    "print_num_layers(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca807a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "829a8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:2%]\"\n",
    ")\n",
    "\n",
    "train_texts = [t for t in train_dataset[\"text\"] if len(t.strip()) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aef309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62074d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    enc = tokenizer(\n",
    "        batch,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    return enc\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_texts,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58cebdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def recovery_finetune(\n",
    "    model,\n",
    "    train_loader,\n",
    "    steps=1000,\n",
    "    lr=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    step = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        if step >= steps:\n",
    "            break\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "\n",
    "        mask = attention_mask[:, 1:].bool()\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = log_probs.gather(\n",
    "            dim=-1, index=labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        loss = -(token_log_probs[mask]).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"[Recovery] step {step} | loss {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3af58abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# 1) Extract features ONCE (same forward pass)\n",
    "feature_mats = extract_ffn_features_single_pass(\n",
    "    base_model, texts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "803a0e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merging PPL: 46.97976365832357\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "print(\"Before merging PPL:\",\n",
    "          compute_perplexity(model, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c033bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdf389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f51e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f64c6dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window [0, 1, 2, 3] | PPL 95623.84\n",
      "Window [1, 2, 3, 4] | PPL 3289.73\n",
      "Window [2, 3, 4, 5] | PPL 4249.91\n",
      "Window [3, 4, 5, 6] | PPL 963.31\n",
      "Window [4, 5, 6, 7] | PPL 1823.28\n",
      "Window [5, 6, 7, 8] | PPL 306.99\n",
      "Window [6, 7, 8, 9] | PPL 212.60\n",
      "Window [7, 8, 9, 10] | PPL 175.50\n",
      "Window [8, 9, 10, 11] | PPL 352.87\n"
     ]
    }
   ],
   "source": [
    "best_ppl = float(\"inf\")\n",
    "best_window = None\n",
    "best_model = None   # <-- store MODEL OBJECT, not state_dict\n",
    "\n",
    "k = 4\n",
    "windows = sliding_windows(len(base_model.transformer.h), k)\n",
    "\n",
    "for window in windows:\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "    perms = compute_adjacent_permutations(feature_mats, window)\n",
    "    merge_k_layers(model, window, perms)\n",
    "\n",
    "    ppl = compute_perplexity(model, texts)\n",
    "    print(f\"Window {window} | PPL {ppl:.2f}\")\n",
    "\n",
    "    if ppl < best_ppl:\n",
    "        best_ppl = ppl\n",
    "        best_window = window\n",
    "        best_model = model   # âœ… keep tied model alive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c89c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights tied? True True\n"
     ]
    }
   ],
   "source": [
    "l0 = best_model.transformer.h[best_window[0]].mlp\n",
    "l1 = best_model.transformer.h[best_window[1]].mlp\n",
    "\n",
    "print(\"Weights tied?\",\n",
    "      l0.c_fc.weight is l1.c_fc.weight,\n",
    "      l0.c_proj.weight is l1.c_proj.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73b39582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before recovery PPL: 175.4980100246964\n",
      "After recovery PPL: 124.45890570165714\n"
     ]
    }
   ],
   "source": [
    "print(\"Before recovery PPL:\",\n",
    "      compute_perplexity(best_model, texts))\n",
    "\n",
    "recovery_finetune(\n",
    "    best_model,\n",
    "    train_loader,\n",
    "    steps=10,\n",
    "    lr=1e-5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"After recovery PPL:\",\n",
    "      compute_perplexity(best_model, texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9116883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PPL after recovery: 124.46\n",
      "TEST PPL after recovery: 169.64\n"
     ]
    }
   ],
   "source": [
    "# ---- TEST EVALUATION ----\n",
    "test_dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"test\"\n",
    ")\n",
    "\n",
    "test_texts = [t for t in test_dataset[\"text\"] if len(t.strip()) > 0][:500]\n",
    "\n",
    "val_ppl = compute_perplexity(best_model, texts)\n",
    "print(f\"Validation PPL after recovery: {val_ppl:.2f}\")\n",
    "\n",
    "test_ppl = compute_perplexity(best_model, test_texts)\n",
    "print(f\"TEST PPL after recovery: {test_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c40c8afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "l0 = best_model.transformer.h[best_window[0]].mlp\n",
    "l1 = best_model.transformer.h[best_window[1]].mlp\n",
    "# print(l0.c_fc.weight)\n",
    "# print(l1.c_fc.weight)\n",
    "print(l0.c_fc.weight is l1.c_fc.weight)   # MUST be True\n",
    "print(l0.c_proj.weight is l1.c_proj.weight)  # MUST be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11164687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2\n",
      " Naive : 124439808\n",
      " Unique: 124439808\n",
      "\n",
      "Merged (tied) model\n",
      " Naive : 110272512\n",
      " Unique: 110272512\n"
     ]
    }
   ],
   "source": [
    "def count_unique_params(model):\n",
    "    seen = set()\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        if id(p) not in seen:\n",
    "            seen.add(id(p))\n",
    "            total += p.numel()\n",
    "    return total\n",
    "\n",
    "def count_params_naive(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"Original GPT-2\")\n",
    "print(\" Naive :\", count_params_naive(base_model))\n",
    "print(\" Unique:\", count_unique_params(base_model))\n",
    "\n",
    "print(\"\\nMerged (tied) model\")\n",
    "print(\" Naive :\", count_params_naive(best_model))\n",
    "print(\" Unique:\", count_unique_params(best_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63717d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated output:\n",
      "India will become global leader in AI because 1. it has the ability to be the world AI leader. 2. it has the ability to be the world AI leader. 3. it has the ability\n"
     ]
    }
   ],
   "source": [
    "prompt = \"India will become global leader in AI because 1. it has\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    sample_out = best_model.generate(**inputs, max_length=40)\n",
    "print(\"\\nGenerated output:\")\n",
    "print(tokenizer.decode(sample_out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63339137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a6f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4bdd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading base model...\n",
      "Extracting FFN features...\n",
      "\n",
      "Searching best window...\n",
      "Window [0, 1, 2, 3] | PPL 3260.79\n",
      "Window [1, 2, 3, 4] | PPL 395.99\n",
      "Window [2, 3, 4, 5] | PPL 128.06\n",
      "Window [3, 4, 5, 6] | PPL 102.01\n",
      "Window [4, 5, 6, 7] | PPL 92.25\n",
      "Window [5, 6, 7, 8] | PPL 91.09\n",
      "Window [6, 7, 8, 9] | PPL 92.88\n",
      "Window [7, 8, 9, 10] | PPL 98.46\n",
      "Window [8, 9, 10, 11] | PPL 350.21\n",
      "\n",
      "Best window: [5, 6, 7, 8]\n",
      "\n",
      "Starting fine-tuning...\n",
      "Step 0 | Loss 6.0397\n",
      "Step 50 | Loss 4.5996\n",
      "Step 100 | Loss 4.4414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Validation PPL: 50.86\n",
      "\n",
      "Generated text:\n",
      "India will become global leader in AI because of the fact of its ability to communicate with the human.\n",
      "\n",
      "The AI is a global leader in the world's AI. It is the most advanced AI\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Soft Shared FFN (Wc + Low-Rank Delta) for GPT-2\n",
    "# No averaging, no MoE, no information loss\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RANK = 8\n",
    "WINDOW_K = 4\n",
    "LAMBDA = 1e-4\n",
    "MAX_TOKENS = 4000\n",
    "EVAL_TEXTS = 200\n",
    "\n",
    "# -------------------------\n",
    "# Tokenizer\n",
    "# -------------------------\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ============================================================\n",
    "# Low-rank FFN module\n",
    "# ============================================================\n",
    "\n",
    "class LowRankLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 compatible low-rank linear:\n",
    "    weight layout = [in_dim, out_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, Wc, bias, rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Wc = Wc          # shared base weight\n",
    "        self.bias = bias\n",
    "\n",
    "        in_dim, out_dim = Wc.shape   # ðŸ”‘ GPT-2 layout\n",
    "\n",
    "        device = Wc.device\n",
    "\n",
    "        # Low-rank delta in SAME layout as Wc\n",
    "        self.A = nn.Parameter(torch.zeros(in_dim, rank, device=device))\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim, device=device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        W_eff = self.Wc + self.A @ self.B   # shape [in_dim, out_dim]\n",
    "        return F.linear(x, W_eff.T, self.bias)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# ============================================================\n",
    "# Feature extraction (c_fc activations)\n",
    "# ============================================================\n",
    "\n",
    "def extract_ffn_features_single_pass(model, texts):\n",
    "    model.eval()\n",
    "    features = {i: [] for i in range(len(model.transformer.h))}\n",
    "    hooks = []\n",
    "\n",
    "    def make_hook(i):\n",
    "        def hook(_, __, out):\n",
    "            features[i].append(out.detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    for i, block in enumerate(model.transformer.h):\n",
    "        hooks.append(block.mlp.c_fc.register_forward_hook(make_hook(i)))\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**enc)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    X = {}\n",
    "    for i in features:\n",
    "        acts = torch.cat(features[i], dim=0)\n",
    "        acts = acts.reshape(-1, acts.shape[-1])\n",
    "        X[i] = acts[:MAX_TOKENS].numpy()\n",
    "\n",
    "    return X\n",
    "\n",
    "# ============================================================\n",
    "# Hungarian alignment\n",
    "# ============================================================\n",
    "\n",
    "def compute_adjacent_permutations(feature_dict, window):\n",
    "    anchor = window[0]\n",
    "    perms = {}\n",
    "    Xa = feature_dict[anchor]\n",
    "\n",
    "    for layer in window[1:]:\n",
    "        Xb = feature_dict[layer]\n",
    "        C = np.corrcoef(Xa, Xb, rowvar=False)\n",
    "        d = Xa.shape[1]\n",
    "        C = C[:d, d:]\n",
    "        _, col = linear_sum_assignment(-C)\n",
    "        perms[layer] = col\n",
    "\n",
    "    return perms\n",
    "\n",
    "def sliding_windows(n, k):\n",
    "    return [list(range(i, i + k)) for i in range(n - k + 1)]\n",
    "\n",
    "# ============================================================\n",
    "# Replace FFNs with soft shared base + low-rank delta\n",
    "# ============================================================\n",
    "\n",
    "def soft_share_ffn_lowrank(model, layers, perms, rank=RANK):\n",
    "    anchor = layers[0]\n",
    "    anchor_mlp = model.transformer.h[anchor].mlp\n",
    "\n",
    "    device = anchor_mlp.c_fc.weight.device\n",
    "\n",
    "    Wc_fc = nn.Parameter(anchor_mlp.c_fc.weight.data.clone().to(device))\n",
    "    bc_fc = nn.Parameter(anchor_mlp.c_fc.bias.data.clone().to(device))\n",
    "    Wc_proj = nn.Parameter(anchor_mlp.c_proj.weight.data.clone().to(device))\n",
    "    bc_proj = nn.Parameter(anchor_mlp.c_proj.bias.data.clone().to(device))\n",
    "\n",
    "\n",
    "    for layer in layers:\n",
    "        mlp = model.transformer.h[layer].mlp\n",
    "        perm = perms.get(layer, None)\n",
    "\n",
    "        # c_fc\n",
    "        mlp.c_fc = LowRankLinear(\n",
    "        Wc=Wc_fc,\n",
    "        bias=bc_fc,\n",
    "        rank=rank\n",
    "        )\n",
    "\n",
    "        mlp.c_proj = LowRankLinear(\n",
    "        Wc=Wc_proj,\n",
    "        bias=bc_proj,\n",
    "        rank=rank\n",
    "        )\n",
    "\n",
    "# ============================================================\n",
    "# Loss helpers\n",
    "# ============================================================\n",
    "\n",
    "def lm_loss(model, batch):\n",
    "    input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits[:, :-1, :]\n",
    "    labels = input_ids[:, 1:]\n",
    "\n",
    "    mask = attention_mask[:, 1:].bool()\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(\n",
    "        dim=-1, index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    return -(token_log_probs[mask]).mean()\n",
    "\n",
    "def consensus_loss(model, layers, lam=LAMBDA):\n",
    "    loss = 0.0\n",
    "    for l in layers:\n",
    "        mlp = model.transformer.h[l].mlp\n",
    "        loss += torch.norm(mlp.c_fc.A @ mlp.c_fc.B, p=\"fro\")**2\n",
    "        loss += torch.norm(mlp.c_proj.A @ mlp.c_proj.B, p=\"fro\")**2\n",
    "    return lam * loss\n",
    "\n",
    "# ============================================================\n",
    "# Perplexity\n",
    "# ============================================================\n",
    "\n",
    "def compute_perplexity(model, texts, batch_size=4):\n",
    "    model.eval()\n",
    "    total_nll = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        enc = tokenizer(\n",
    "            texts[i:i + batch_size],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(enc[\"input_ids\"]).logits[:, :-1, :]\n",
    "            labels = enc[\"input_ids\"][:, 1:]\n",
    "            mask = enc[\"attention_mask\"][:, 1:].bool()\n",
    "\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            lp = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        total_nll += -(lp[mask]).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    return math.exp(total_nll / total_tokens)\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading data...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "texts = [t for t in dataset[\"text\"] if len(t.strip()) > 0][:EVAL_TEXTS]\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "\n",
    "print(\"Extracting FFN features...\")\n",
    "features = extract_ffn_features_single_pass(base_model, texts)\n",
    "\n",
    "windows = sliding_windows(len(base_model.transformer.h), WINDOW_K)\n",
    "\n",
    "best_ppl = float(\"inf\")\n",
    "best_window = None\n",
    "\n",
    "print(\"\\nSearching best window...\")\n",
    "for w in windows:\n",
    "    test_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "    perms = compute_adjacent_permutations(features, w)\n",
    "    soft_share_ffn_lowrank(test_model, w, perms)\n",
    "\n",
    "    ppl = compute_perplexity(test_model, texts)\n",
    "    print(f\"Window {w} | PPL {ppl:.2f}\")\n",
    "\n",
    "    if ppl < best_ppl:\n",
    "        best_ppl = ppl\n",
    "        best_window = w\n",
    "\n",
    "print(\"\\nBest window:\", best_window)\n",
    "\n",
    "# ============================================================\n",
    "# Fine-tuning\n",
    "# ============================================================\n",
    "\n",
    "train_ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:2%]\")\n",
    "train_texts = [t for t in train_ds[\"text\"] if len(t.strip()) > 0]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "train_loader = DataLoader(train_texts, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "perms = compute_adjacent_permutations(features, best_window)\n",
    "soft_share_ffn_lowrank(model, best_window, perms)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "print(\"\\nStarting fine-tuning...\")\n",
    "model.train()\n",
    "for step, batch in enumerate(train_loader):\n",
    "    if step >= 300:\n",
    "        break\n",
    "\n",
    "    loss = lm_loss(model, batch)\n",
    "    loss += consensus_loss(model, best_window)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step} | Loss {loss.item():.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# Final evaluation\n",
    "# ============================================================\n",
    "\n",
    "val_ppl = compute_perplexity(model, texts)\n",
    "print(f\"\\nFinal Validation PPL: {val_ppl:.2f}\")\n",
    "\n",
    "prompt = \"India will become global leader in AI because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_length=40)\n",
    "\n",
    "print(\"\\nGenerated text:\")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e61a28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PPL after recovery: 68.58\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"test\"\n",
    ")\n",
    "\n",
    "test_texts = [t for t in test_dataset[\"text\"] if len(t.strip()) > 0][:500]\n",
    "test_ppl = compute_perplexity(model, test_texts)\n",
    "print(f\"TEST PPL after recovery: {test_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5ed01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c0c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3817cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b8d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
