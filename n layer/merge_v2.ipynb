{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca48668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from datasets import load_dataset\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc46d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372b7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44dfa2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ffn_features_single_pass(model, texts, max_tokens=4000):\n",
    "    \"\"\"\n",
    "    Extract FFN pre-activation features (c_fc outputs)\n",
    "    for all layers from a SINGLE forward pass,\n",
    "    excluding padding tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    features = {i: [] for i in range(len(model.transformer.h))}\n",
    "    hooks = []\n",
    "\n",
    "    def make_hook(layer_idx):\n",
    "        def hook(module, inp, out):\n",
    "            # out: [batch, seq, d_ff]\n",
    "            features[layer_idx].append(out.detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    # Register hooks ONLY on c_fc\n",
    "    for i, block in enumerate(model.transformer.h):\n",
    "        hooks.append(\n",
    "            block.mlp.c_fc.register_forward_hook(make_hook(i))\n",
    "        )\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(device)\n",
    "\n",
    "    attention_mask = enc[\"attention_mask\"]  # [B, T]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**enc)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    X = {}\n",
    "    for i in features:\n",
    "        # [B, T, d]\n",
    "        # acts = torch.cat(features[i], dim=0)\n",
    "\n",
    "        # # mask out padding tokens\n",
    "        # mask = attention_mask.bool().unsqueeze(-1)  # [B, T, 1]\n",
    "        # acts = acts[mask.expand_as(acts)].view(-1, acts.shape[-1])\n",
    "\n",
    "        \n",
    "        acts = torch.cat(features[i], dim=0)  # acts is on CPU\n",
    "\n",
    "        mask = attention_mask.bool().unsqueeze(-1).cpu()  # <-- FIX\n",
    "\n",
    "        acts = acts[mask.expand_as(acts)].view(-1, acts.shape[-1])\n",
    "\n",
    "        if acts.shape[0] > max_tokens:\n",
    "            acts = acts[:max_tokens]\n",
    "\n",
    "        X[i] = acts.numpy()\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd3781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "def compute_adjacent_permutations(feature_dict, window_layers):\n",
    "    \"\"\"\n",
    "    Compute permutations ONLY between anchor and adjacent layers\n",
    "    inside a window.\n",
    "    \"\"\"\n",
    "    anchor = window_layers[0]\n",
    "    perms = {}\n",
    "\n",
    "    X_anchor = feature_dict[anchor]\n",
    "\n",
    "    for layer in window_layers[1:]:\n",
    "        X_other = feature_dict[layer]\n",
    "\n",
    "        # Pearson correlation (Eq. 1 in paper)\n",
    "        C = np.corrcoef(X_anchor, X_other, rowvar=False)\n",
    "        d = X_anchor.shape[1]\n",
    "        C = C[:d, d:]\n",
    "\n",
    "        _, col_ind = linear_sum_assignment(-C)\n",
    "        perms[layer] = col_ind\n",
    "\n",
    "    return perms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9600e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(n_layers, k):\n",
    "    \"\"\"\n",
    "    Generate sliding windows of k adjacent layers.\n",
    "    \"\"\"\n",
    "    return [list(range(i, i + k)) for i in range(n_layers - k + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0665f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6904fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_ffn(model, layer, perm):\n",
    "    block = model.transformer.h[layer].mlp\n",
    "    perm = torch.tensor(perm, dtype=torch.long, device=device)\n",
    "\n",
    "    # GPT-2 weight shapes are transposed in your setup:\n",
    "    # c_fc:   [768, 3072]\n",
    "    # c_proj: [3072, 768]\n",
    "    with torch.no_grad():\n",
    "        block.c_fc.weight[:] = block.c_fc.weight[:, perm]\n",
    "        block.c_fc.bias[:]   = block.c_fc.bias[perm]\n",
    "        block.c_proj.weight[:] = block.c_proj.weight[perm, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c9ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def merge_k_layers(model, layers, perms):\n",
    "    \"\"\"\n",
    "    Paper-faithful FFN merging with TRUE weight tying.\n",
    "    - No in-place permutation\n",
    "    - Logical permutation inside averaging\n",
    "    - Shared nn.Parameter objects\n",
    "    \"\"\"\n",
    "\n",
    "    anchor = layers[0]\n",
    "    anchor_mlp = model.transformer.h[anchor].mlp\n",
    "\n",
    "    device = anchor_mlp.c_fc.weight.device\n",
    "    k = len(layers)\n",
    "\n",
    "    # Start from anchor weights\n",
    "    W_in  = anchor_mlp.c_fc.weight.data.clone()\n",
    "    b_in  = anchor_mlp.c_fc.bias.data.clone()\n",
    "    W_out = anchor_mlp.c_proj.weight.data.clone()\n",
    "    b_out = anchor_mlp.c_proj.bias.data.clone()\n",
    "\n",
    "    # Accumulate aligned weights (NO mutation)\n",
    "    for layer in layers[1:]:\n",
    "        mlp = model.transformer.h[layer].mlp\n",
    "        perm = torch.tensor(perms[layer], device=device)\n",
    "\n",
    "        # Logical permutation (as in Eq. 3â€“6)\n",
    "        W_in  += mlp.c_fc.weight[:, perm]\n",
    "        b_in  += mlp.c_fc.bias[perm]\n",
    "        W_out += mlp.c_proj.weight[perm, :]\n",
    "        b_out += mlp.c_proj.bias\n",
    "\n",
    "    # Average\n",
    "    W_in  /= k\n",
    "    b_in  /= k\n",
    "    W_out /= k\n",
    "    b_out /= k\n",
    "\n",
    "    # Create SHARED parameters (true tying)\n",
    "    shared_c_fc_weight   = nn.Parameter(W_in)\n",
    "    shared_c_fc_bias     = nn.Parameter(b_in)\n",
    "    shared_c_proj_weight = nn.Parameter(W_out)\n",
    "    shared_c_proj_bias   = nn.Parameter(b_out)\n",
    "\n",
    "    # Tie all layers in the window\n",
    "    for layer in layers:\n",
    "        mlp = model.transformer.h[layer].mlp\n",
    "\n",
    "        mlp.c_fc.weight = shared_c_fc_weight\n",
    "        mlp.c_fc.bias   = shared_c_fc_bias\n",
    "\n",
    "        mlp.c_proj.weight = shared_c_proj_weight\n",
    "        mlp.c_proj.bias   = shared_c_proj_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0830f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "def compute_perplexity(model, texts, batch_size=4, max_length=128):\n",
    "    model.eval()\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ).to(device)\n",
    "\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, :-1, :]\n",
    "            labels = input_ids[:, 1:]\n",
    "\n",
    "        # mask padding tokens\n",
    "        mask = attention_mask[:, 1:].bool()\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = log_probs.gather(\n",
    "            dim=-1, index=labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        total_nll += -(token_log_probs[mask]).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    return math.exp(total_nll / total_tokens)\n",
    "\n",
    "# # merged_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "# # merge_k_layers(merged_model, window, perms)\n",
    "\n",
    "# merged_ppl = compute_perplexity(model, texts)\n",
    "\n",
    "# print(f\"Merged (window={window}) PPL: {merged_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271ccb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cc52aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline GPT-2 PPL: 46.98\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\"\n",
    ")\n",
    "texts = [t for t in dataset[\"text\"] if len(t.strip()) > 0][:200]\n",
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "base_ppl = compute_perplexity(base_model, texts)\n",
    "\n",
    "print(f\"Baseline GPT-2 PPL: {base_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6085f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Before recovery PPL:\",\n",
    "#           compute_perplexity(model, texts))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb754fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transformer layers: 12\n"
     ]
    }
   ],
   "source": [
    "def print_num_layers(model):\n",
    "    num_layers = len(model.transformer.h)\n",
    "    print(f\"Number of transformer layers: {num_layers}\")\n",
    "print_num_layers(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b78396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2\n",
      "  Naive params  : 124,439,808\n",
      "  Unique params : 124,439,808\n",
      "  Saved params  : 0\n",
      "Merged Model\n",
      "  Naive params  : 119,717,376\n",
      "  Unique params : 119,717,376\n",
      "  Saved params  : 0\n"
     ]
    }
   ],
   "source": [
    "# def count_unique_params(model):\n",
    "#     seen = set()\n",
    "#     total = 0\n",
    "#     for p in model.parameters():\n",
    "#         if id(p) not in seen:\n",
    "#             seen.add(id(p))\n",
    "#             total += p.numel()\n",
    "#     return total\n",
    "# def count_params_naive(model):\n",
    "#     return sum(p.numel() for p in model.parameters())\n",
    "# def print_param_stats(model, name=\"model\"):\n",
    "#     naive = count_params_naive(model)\n",
    "#     unique = count_unique_params(model)\n",
    "#     print(f\"{name}\")\n",
    "#     print(f\"  Naive params  : {naive:,}\")\n",
    "#     print(f\"  Unique params : {unique:,}\")\n",
    "#     print(f\"  Saved params  : {naive - unique:,}\")\n",
    "# print_param_stats(base_model, \"Original GPT-2\")\n",
    "# print_param_stats(model, \"Merged Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "829a8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:2%]\"\n",
    ")\n",
    "\n",
    "train_texts = [t for t in train_dataset[\"text\"] if len(t.strip()) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aef309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62074d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    enc = tokenizer(\n",
    "        batch,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    return enc\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_texts,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58cebdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def recovery_finetune(\n",
    "    model,\n",
    "    train_loader,\n",
    "    steps=1000,\n",
    "    lr=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    step = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        if step >= steps:\n",
    "            break\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "\n",
    "        mask = attention_mask[:, 1:].bool()\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = log_probs.gather(\n",
    "            dim=-1, index=labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        loss = -(token_log_probs[mask]).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"[Recovery] step {step} | loss {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3af58abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# 1) Extract features ONCE (same forward pass)\n",
    "feature_mats = extract_ffn_features_single_pass(\n",
    "    base_model, texts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "803a0e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merging PPL: 46.97976365832357\n",
      "Window [0, 1] | PPL 196.45\n",
      "Window [1, 2] | PPL 101.00\n",
      "Window [2, 3] | PPL 86.62\n",
      "Window [3, 4] | PPL 74.25\n",
      "Window [4, 5] | PPL 80.09\n",
      "Window [5, 6] | PPL 92.55\n",
      "Window [6, 7] | PPL 60.75\n",
      "Window [7, 8] | PPL 58.43\n",
      "Window [8, 9] | PPL 57.56\n",
      "Window [9, 10] | PPL 61.13\n",
      "Window [10, 11] | PPL 66.76\n",
      "\n",
      "BEST WINDOW: [8, 9]\n",
      "BEST PPL AFTER MERGING: 57.56\n"
     ]
    }
   ],
   "source": [
    "best_ppl = float(\"inf\")\n",
    "best_window = None\n",
    "best_state_dict = None\n",
    "\n",
    "k = 2\n",
    "windows = sliding_windows(len(base_model.transformer.h), k)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "print(\"Before merging PPL:\",\n",
    "          compute_perplexity(model, texts))\n",
    "\n",
    "for window in windows:\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "    perms = compute_adjacent_permutations(feature_mats, window)\n",
    "    merge_k_layers(model, window, perms)\n",
    "\n",
    "    ppl = compute_perplexity(model, texts)\n",
    "    print(f\"Window {window} | PPL {ppl:.2f}\")\n",
    "\n",
    "    if ppl < best_ppl:\n",
    "        best_ppl = ppl\n",
    "        best_window = window\n",
    "        best_state_dict = {\n",
    "            k: v.detach().cpu().clone()\n",
    "            for k, v in model.state_dict().items()\n",
    "        }\n",
    "\n",
    "\n",
    "best_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "best_model.load_state_dict(best_state_dict)\n",
    "\n",
    "print(f\"\\nBEST WINDOW: {best_window}\")\n",
    "print(f\"BEST PPL AFTER MERGING: {best_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6b94c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "train_model.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7424aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training PPL: 57.563756951116204\n",
      "After training PPL: 50.98165648508262\n"
     ]
    }
   ],
   "source": [
    "print(\"Before training PPL:\",\n",
    "          compute_perplexity(train_model, texts))\n",
    "\n",
    "recovery_finetune(\n",
    "    train_model,\n",
    "    train_loader,\n",
    "    steps=10,   # start small\n",
    "    lr=1e-5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"After training PPL:\",\n",
    "        compute_perplexity(train_model, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8a9dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PPL after recovery: 50.98\n",
      "TEST PPL after recovery: 70.07\n"
     ]
    }
   ],
   "source": [
    "# ---- TEST EVALUATION ----\n",
    "test_dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"test\"\n",
    ")\n",
    "\n",
    "test_texts = [t for t in test_dataset[\"text\"] if len(t.strip()) > 0][:500]\n",
    "\n",
    "val_ppl = compute_perplexity(train_model, texts)\n",
    "print(f\"Validation PPL after recovery: {val_ppl:.2f}\")\n",
    "\n",
    "test_ppl = compute_perplexity(train_model, test_texts)\n",
    "print(f\"TEST PPL after recovery: {test_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7587705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training PPL: 67.12803486098032\n"
     ]
    }
   ],
   "source": [
    "print(\"After training PPL:\",\n",
    "        compute_perplexity(train_model, train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8014a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated output:\n",
      "India will become global leader in AI because 1. it has the world's first AI-enabled smart car, and 2. it has the world's first AI-enabled smart car.\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "prompt = \"India will become global leader in AI because 1. it has\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    sample_out = train_model.generate(**inputs, max_length=40)\n",
    "print(\"\\nGenerated output:\")\n",
    "print(tokenizer.decode(sample_out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36c81f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "l0 = model.transformer.h[window[0]].mlp\n",
    "l1 = model.transformer.h[window[1]].mlp\n",
    "\n",
    "print(l0.c_fc.weight is l1.c_fc.weight)   # MUST be True\n",
    "print(l0.c_proj.weight is l1.c_proj.weight)  # MUST be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in windows:\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "    perms = compute_adjacent_permutations(feature_mats, window)\n",
    "    merge_k_layers(model, window, perms)\n",
    "\n",
    "    print(\"Before recovery PPL:\",\n",
    "          compute_perplexity(model, texts))\n",
    "\n",
    "    recovery_finetune(\n",
    "        model,\n",
    "        train_loader,\n",
    "        steps=1000,   # start small\n",
    "        lr=1e-5,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(\"After recovery PPL:\",\n",
    "          compute_perplexity(model, texts))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
