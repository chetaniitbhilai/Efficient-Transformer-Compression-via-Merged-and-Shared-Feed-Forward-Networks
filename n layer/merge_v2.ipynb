{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca48668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from datasets import load_dataset\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc46d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372b7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44dfa2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ffn_features_single_pass(model, texts, max_tokens=4000):\n",
    "    \"\"\"\n",
    "    Extract FFN pre-activation features (c_fc outputs)\n",
    "    for all layers from a SINGLE forward pass,\n",
    "    excluding padding tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    features = {i: [] for i in range(len(model.transformer.h))}\n",
    "    hooks = []\n",
    "\n",
    "    def make_hook(layer_idx):\n",
    "        def hook(module, inp, out):\n",
    "            # out: [batch, seq, d_ff]\n",
    "            features[layer_idx].append(out.detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    # Register hooks ONLY on c_fc\n",
    "    for i, block in enumerate(model.transformer.h):\n",
    "        hooks.append(\n",
    "            block.mlp.c_fc.register_forward_hook(make_hook(i))\n",
    "        )\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(device)\n",
    "\n",
    "    attention_mask = enc[\"attention_mask\"]  # [B, T]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**enc)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    X = {}\n",
    "    for i in features:\n",
    "        # [B, T, d]\n",
    "        # acts = torch.cat(features[i], dim=0)\n",
    "\n",
    "        # # mask out padding tokens\n",
    "        # mask = attention_mask.bool().unsqueeze(-1)  # [B, T, 1]\n",
    "        # acts = acts[mask.expand_as(acts)].view(-1, acts.shape[-1])\n",
    "\n",
    "        \n",
    "        acts = torch.cat(features[i], dim=0)  # acts is on CPU\n",
    "\n",
    "        mask = attention_mask.bool().unsqueeze(-1).cpu()  # <-- FIX\n",
    "\n",
    "        acts = acts[mask.expand_as(acts)].view(-1, acts.shape[-1])\n",
    "\n",
    "        if acts.shape[0] > max_tokens:\n",
    "            acts = acts[:max_tokens]\n",
    "\n",
    "        X[i] = acts.numpy()\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd3781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "def compute_adjacent_permutations(feature_dict, window_layers):\n",
    "    \"\"\"\n",
    "    Compute permutations ONLY between anchor and adjacent layers\n",
    "    inside a window.\n",
    "    \"\"\"\n",
    "    anchor = window_layers[0]\n",
    "    perms = {}\n",
    "\n",
    "    X_anchor = feature_dict[anchor]\n",
    "\n",
    "    for layer in window_layers[1:]:\n",
    "        X_other = feature_dict[layer]\n",
    "\n",
    "        # Pearson correlation (Eq. 1 in paper)\n",
    "        C = np.corrcoef(X_anchor, X_other, rowvar=False)\n",
    "        d = X_anchor.shape[1]\n",
    "        C = C[:d, d:]\n",
    "\n",
    "        _, col_ind = linear_sum_assignment(-C)\n",
    "        perms[layer] = col_ind\n",
    "\n",
    "    return perms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9600e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(n_layers, k):\n",
    "    \"\"\"\n",
    "    Generate sliding windows of k adjacent layers.\n",
    "    \"\"\"\n",
    "    return [list(range(i, i + k)) for i in range(n_layers - k + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0665f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6904fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_ffn(model, layer, perm):\n",
    "    block = model.transformer.h[layer].mlp\n",
    "    perm = torch.tensor(perm, dtype=torch.long, device=device)\n",
    "\n",
    "    # GPT-2 weight shapes are transposed in your setup:\n",
    "    # c_fc:   [768, 3072]\n",
    "    # c_proj: [3072, 768]\n",
    "    with torch.no_grad():\n",
    "        block.c_fc.weight[:] = block.c_fc.weight[:, perm]\n",
    "        block.c_fc.bias[:]   = block.c_fc.bias[perm]\n",
    "        block.c_proj.weight[:] = block.c_proj.weight[perm, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c9ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def merge_k_layers(model, layers, perms):\n",
    "    \"\"\"\n",
    "    Paper-faithful FFN merging with TRUE weight tying.\n",
    "    - No in-place permutation\n",
    "    - Logical permutation inside averaging\n",
    "    - Shared nn.Parameter objects\n",
    "    \"\"\"\n",
    "\n",
    "    anchor = layers[0]\n",
    "    anchor_mlp = model.transformer.h[anchor].mlp\n",
    "\n",
    "    device = anchor_mlp.c_fc.weight.device\n",
    "    k = len(layers)\n",
    "\n",
    "    # Start from anchor weights\n",
    "    W_in  = anchor_mlp.c_fc.weight.data.clone()\n",
    "    b_in  = anchor_mlp.c_fc.bias.data.clone()\n",
    "    W_out = anchor_mlp.c_proj.weight.data.clone()\n",
    "    b_out = anchor_mlp.c_proj.bias.data.clone()\n",
    "\n",
    "    # Accumulate aligned weights (NO mutation)\n",
    "    for layer in layers[1:]:\n",
    "        mlp = model.transformer.h[layer].mlp\n",
    "        perm = torch.tensor(perms[layer], device=device)\n",
    "\n",
    "        # Logical permutation (as in Eq. 3–6)\n",
    "        W_in  += mlp.c_fc.weight[:, perm]\n",
    "        b_in  += mlp.c_fc.bias[perm]\n",
    "        W_out += mlp.c_proj.weight[perm, :]\n",
    "        b_out += mlp.c_proj.bias\n",
    "\n",
    "    # Average\n",
    "    W_in  /= k\n",
    "    b_in  /= k\n",
    "    W_out /= k\n",
    "    b_out /= k\n",
    "\n",
    "    # Create SHARED parameters (true tying)\n",
    "    shared_c_fc_weight   = nn.Parameter(W_in)\n",
    "    shared_c_fc_bias     = nn.Parameter(b_in)\n",
    "    shared_c_proj_weight = nn.Parameter(W_out)\n",
    "    shared_c_proj_bias   = nn.Parameter(b_out)\n",
    "\n",
    "    # Tie all layers in the window\n",
    "    for layer in layers:\n",
    "        mlp = model.transformer.h[layer].mlp\n",
    "\n",
    "        mlp.c_fc.weight = shared_c_fc_weight\n",
    "        mlp.c_fc.bias   = shared_c_fc_bias\n",
    "\n",
    "        mlp.c_proj.weight = shared_c_proj_weight\n",
    "        mlp.c_proj.bias   = shared_c_proj_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0830f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "def compute_perplexity(model, texts, batch_size=4, max_length=128):\n",
    "    model.eval()\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ).to(device)\n",
    "\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, :-1, :]\n",
    "            labels = input_ids[:, 1:]\n",
    "\n",
    "        # mask padding tokens\n",
    "        mask = attention_mask[:, 1:].bool()\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = log_probs.gather(\n",
    "            dim=-1, index=labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        total_nll += -(token_log_probs[mask]).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    return math.exp(total_nll / total_tokens)\n",
    "\n",
    "# # merged_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "# # merge_k_layers(merged_model, window, perms)\n",
    "\n",
    "# merged_ppl = compute_perplexity(model, texts)\n",
    "\n",
    "# print(f\"Merged (window={window}) PPL: {merged_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271ccb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc52aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline GPT-2 PPL: 46.98\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\"\n",
    ")\n",
    "texts = [t for t in dataset[\"text\"] if len(t.strip()) > 0][:200]\n",
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "base_ppl = compute_perplexity(base_model, texts)\n",
    "\n",
    "print(f\"Baseline GPT-2 PPL: {base_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6085f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Before recovery PPL:\",\n",
    "#           compute_perplexity(model, texts))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb754fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transformer layers: 12\n"
     ]
    }
   ],
   "source": [
    "def print_num_layers(model):\n",
    "    num_layers = len(model.transformer.h)\n",
    "    print(f\"Number of transformer layers: {num_layers}\")\n",
    "print_num_layers(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b78396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2\n",
      "  Naive params  : 124,439,808\n",
      "  Unique params : 124,439,808\n",
      "  Saved params  : 0\n",
      "Merged Model\n",
      "  Naive params  : 119,717,376\n",
      "  Unique params : 119,717,376\n",
      "  Saved params  : 0\n"
     ]
    }
   ],
   "source": [
    "# def count_unique_params(model):\n",
    "#     seen = set()\n",
    "#     total = 0\n",
    "#     for p in model.parameters():\n",
    "#         if id(p) not in seen:\n",
    "#             seen.add(id(p))\n",
    "#             total += p.numel()\n",
    "#     return total\n",
    "# def count_params_naive(model):\n",
    "#     return sum(p.numel() for p in model.parameters())\n",
    "# def print_param_stats(model, name=\"model\"):\n",
    "#     naive = count_params_naive(model)\n",
    "#     unique = count_unique_params(model)\n",
    "#     print(f\"{name}\")\n",
    "#     print(f\"  Naive params  : {naive:,}\")\n",
    "#     print(f\"  Unique params : {unique:,}\")\n",
    "#     print(f\"  Saved params  : {naive - unique:,}\")\n",
    "# print_param_stats(base_model, \"Original GPT-2\")\n",
    "# print_param_stats(model, \"Merged Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca807a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "829a8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:2%]\"\n",
    ")\n",
    "\n",
    "train_texts = [t for t in train_dataset[\"text\"] if len(t.strip()) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aef309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62074d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    enc = tokenizer(\n",
    "        batch,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    return enc\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_texts,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58cebdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def recovery_finetune(\n",
    "    model,\n",
    "    train_loader,\n",
    "    steps=1000,\n",
    "    lr=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    step = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        if step >= steps:\n",
    "            break\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "\n",
    "        mask = attention_mask[:, 1:].bool()\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = log_probs.gather(\n",
    "            dim=-1, index=labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        loss = -(token_log_probs[mask]).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"[Recovery] step {step} | loss {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3af58abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# 1) Extract features ONCE (same forward pass)\n",
    "feature_mats = extract_ffn_features_single_pass(\n",
    "    base_model, texts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "803a0e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merging PPL: 46.97976365832357\n",
      "Window [0, 1] | PPL 196.45\n",
      "Window [1, 2] | PPL 101.00\n",
      "Window [2, 3] | PPL 86.62\n",
      "Window [3, 4] | PPL 74.25\n",
      "Window [4, 5] | PPL 80.09\n",
      "Window [5, 6] | PPL 92.55\n",
      "Window [6, 7] | PPL 60.75\n",
      "Window [7, 8] | PPL 58.43\n",
      "Window [8, 9] | PPL 57.56\n",
      "Window [9, 10] | PPL 61.13\n",
      "Window [10, 11] | PPL 66.76\n",
      "\n",
      "BEST WINDOW: [8, 9]\n",
      "BEST PPL AFTER MERGING: 57.56\n"
     ]
    }
   ],
   "source": [
    "best_ppl = float(\"inf\")\n",
    "best_window = None\n",
    "best_state_dict = None\n",
    "\n",
    "k = 2\n",
    "windows = sliding_windows(len(base_model.transformer.h), k)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "print(\"Before merging PPL:\",\n",
    "          compute_perplexity(model, texts))\n",
    "\n",
    "for window in windows:\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "    perms = compute_adjacent_permutations(feature_mats, window)\n",
    "    merge_k_layers(model, window, perms)\n",
    "\n",
    "    ppl = compute_perplexity(model, texts)\n",
    "    print(f\"Window {window} | PPL {ppl:.2f}\")\n",
    "\n",
    "    if ppl < best_ppl:\n",
    "        best_ppl = ppl\n",
    "        best_window = window\n",
    "        best_state_dict = {\n",
    "            k: v.detach().cpu().clone()\n",
    "            for k, v in model.state_dict().items()\n",
    "        }\n",
    "\n",
    "\n",
    "best_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "best_model.load_state_dict(best_state_dict)\n",
    "\n",
    "print(f\"\\nBEST WINDOW: {best_window}\")\n",
    "print(f\"BEST PPL AFTER MERGING: {best_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6b94c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "train_model.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7424aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training PPL: 57.563756951116204\n",
      "After training PPL: 50.988636270115435\n"
     ]
    }
   ],
   "source": [
    "print(\"Before training PPL:\",\n",
    "          compute_perplexity(train_model, texts))\n",
    "\n",
    "recovery_finetune(\n",
    "    train_model,\n",
    "    train_loader,\n",
    "    steps=10,   # start small\n",
    "    lr=1e-5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"After training PPL:\",\n",
    "        compute_perplexity(train_model, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8a9dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation PPL after recovery: 50.99\n",
      "TEST PPL after recovery: 70.06\n"
     ]
    }
   ],
   "source": [
    "# ---- TEST EVALUATION ----\n",
    "test_dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"test\"\n",
    ")\n",
    "\n",
    "test_texts = [t for t in test_dataset[\"text\"] if len(t.strip()) > 0][:500]\n",
    "\n",
    "val_ppl = compute_perplexity(train_model, texts)\n",
    "print(f\"Validation PPL after recovery: {val_ppl:.2f}\")\n",
    "\n",
    "test_ppl = compute_perplexity(train_model, test_texts)\n",
    "print(f\"TEST PPL after recovery: {test_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7587705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training PPL: 66.83528990070141\n"
     ]
    }
   ],
   "source": [
    "print(\"After training PPL:\",\n",
    "        compute_perplexity(train_model, train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8014a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated output:\n",
      "India will become global leader in AI because 1. it has the ability to make decisions in the world and 2. it has the ability to make decisions in the world.\n",
      "\n",
      "The AI system is\n"
     ]
    }
   ],
   "source": [
    "prompt = \"India will become global leader in AI because 1. it has\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    sample_out = train_model.generate(**inputs, max_length=40)\n",
    "print(\"\\nGenerated output:\")\n",
    "print(tokenizer.decode(sample_out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36c81f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "l0 = model.transformer.h[window[0]].mlp\n",
    "l1 = model.transformer.h[window[1]].mlp\n",
    "\n",
    "print(l0.c_fc.weight is l1.c_fc.weight)   # MUST be True\n",
    "print(l0.c_proj.weight is l1.c_proj.weight)  # MUST be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6dd79cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2\n",
      "  Naive params  : 124,439,808\n",
      "  Unique params : 124,439,808\n",
      "  Saved params  : 0\n",
      "Merged Model\n",
      "  Naive params  : 119,717,376\n",
      "  Unique params : 119,717,376\n",
      "  Saved params  : 0\n"
     ]
    }
   ],
   "source": [
    "def count_unique_params(model):\n",
    "    seen = set()\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        if id(p) not in seen:\n",
    "            seen.add(id(p))\n",
    "            total += p.numel()\n",
    "    return total\n",
    "def count_params_naive(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "def print_param_stats(model, name=\"model\"):\n",
    "    naive = count_params_naive(model)\n",
    "    unique = count_unique_params(model)\n",
    "    print(f\"{name}\")\n",
    "    print(f\"  Naive params  : {naive:,}\")\n",
    "    print(f\"  Unique params : {unique:,}\")\n",
    "    print(f\"  Saved params  : {naive - unique:,}\")\n",
    "print_param_stats(base_model, \"Original GPT-2\")\n",
    "print_param_stats(model, \"Merged Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c033bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdf389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f51e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f64c6dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window [0, 1] | PPL 196.45\n",
      "Window [1, 2] | PPL 101.00\n",
      "Window [2, 3] | PPL 86.62\n",
      "Window [3, 4] | PPL 74.25\n",
      "Window [4, 5] | PPL 80.09\n",
      "Window [5, 6] | PPL 92.55\n",
      "Window [6, 7] | PPL 60.75\n",
      "Window [7, 8] | PPL 58.43\n",
      "Window [8, 9] | PPL 57.56\n",
      "Window [9, 10] | PPL 61.13\n",
      "Window [10, 11] | PPL 66.76\n"
     ]
    }
   ],
   "source": [
    "best_ppl = float(\"inf\")\n",
    "best_window = None\n",
    "best_model = None   # <-- store MODEL OBJECT, not state_dict\n",
    "\n",
    "k = 2\n",
    "windows = sliding_windows(len(base_model.transformer.h), k)\n",
    "\n",
    "for window in windows:\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "    perms = compute_adjacent_permutations(feature_mats, window)\n",
    "    merge_k_layers(model, window, perms)\n",
    "\n",
    "    ppl = compute_perplexity(model, texts)\n",
    "    print(f\"Window {window} | PPL {ppl:.2f}\")\n",
    "\n",
    "    if ppl < best_ppl:\n",
    "        best_ppl = ppl\n",
    "        best_window = window\n",
    "        best_model = model   # ✅ keep tied model alive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c89c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights tied? True True\n"
     ]
    }
   ],
   "source": [
    "l0 = best_model.transformer.h[best_window[0]].mlp\n",
    "l1 = best_model.transformer.h[best_window[1]].mlp\n",
    "\n",
    "print(\"Weights tied?\",\n",
    "      l0.c_fc.weight is l1.c_fc.weight,\n",
    "      l0.c_proj.weight is l1.c_proj.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73b39582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before recovery PPL: 57.563756951116204\n",
      "After recovery PPL: 50.34575648285173\n"
     ]
    }
   ],
   "source": [
    "print(\"Before recovery PPL:\",\n",
    "      compute_perplexity(best_model, texts))\n",
    "\n",
    "recovery_finetune(\n",
    "    best_model,\n",
    "    train_loader,\n",
    "    steps=10,\n",
    "    lr=1e-5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"After recovery PPL:\",\n",
    "      compute_perplexity(best_model, texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c8afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0113,  0.0835,  0.0336,  ..., -0.0634, -0.1078, -0.0648],\n",
      "        [-0.0199, -0.0391,  0.0831,  ..., -0.1936,  0.0187,  0.0800],\n",
      "        [ 0.1375, -0.0130,  0.0311,  ...,  0.0701, -0.0677,  0.1381],\n",
      "        ...,\n",
      "        [ 0.0741, -0.0061, -0.1858,  ...,  0.0620,  0.0780,  0.1476],\n",
      "        [ 0.0216,  0.1608,  0.0733,  ..., -0.0876,  0.0063,  0.0154],\n",
      "        [-0.0899,  0.0435,  0.0539,  ..., -0.0951,  0.1215,  0.0606]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0113,  0.0835,  0.0336,  ..., -0.0634, -0.1078, -0.0648],\n",
      "        [-0.0199, -0.0391,  0.0831,  ..., -0.1936,  0.0187,  0.0800],\n",
      "        [ 0.1375, -0.0130,  0.0311,  ...,  0.0701, -0.0677,  0.1381],\n",
      "        ...,\n",
      "        [ 0.0741, -0.0061, -0.1858,  ...,  0.0620,  0.0780,  0.1476],\n",
      "        [ 0.0216,  0.1608,  0.0733,  ..., -0.0876,  0.0063,  0.0154],\n",
      "        [-0.0899,  0.0435,  0.0539,  ..., -0.0951,  0.1215,  0.0606]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "l0 = best_model.transformer.h[best_window[0]].mlp\n",
    "l1 = best_model.transformer.h[best_window[1]].mlp\n",
    "# print(l0.c_fc.weight)\n",
    "# print(l1.c_fc.weight)\n",
    "print(l0.c_fc.weight is l1.c_fc.weight)   # MUST be True\n",
    "print(l0.c_proj.weight is l1.c_proj.weight)  # MUST be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11164687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2\n",
      " Naive : 124439808\n",
      " Unique: 124439808\n",
      "\n",
      "Merged (tied) model\n",
      " Naive : 119717376\n",
      " Unique: 119717376\n"
     ]
    }
   ],
   "source": [
    "def count_unique_params(model):\n",
    "    seen = set()\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        if id(p) not in seen:\n",
    "            seen.add(id(p))\n",
    "            total += p.numel()\n",
    "    return total\n",
    "\n",
    "def count_params_naive(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"Original GPT-2\")\n",
    "print(\" Naive :\", count_params_naive(base_model))\n",
    "print(\" Unique:\", count_unique_params(base_model))\n",
    "\n",
    "print(\"\\nMerged (tied) model\")\n",
    "print(\" Naive :\", count_params_naive(best_model))\n",
    "print(\" Unique:\", count_unique_params(best_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63717d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63339137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a6f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bdd55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61a28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d209a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5ed01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c0c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3817cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96944603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (1.15.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (2.2.6)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from accelerate) (2.9.1+cu128)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rohan kumarmishra\\miniconda3\\envs\\aml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading sentencepiece-0.2.1-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.5/1.1 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 3.6 MB/s  0:00:00\n",
      "Installing collected packages: sentencepiece, accelerate\n",
      "\n",
      "   -------------------- ------------------- 1/2 [accelerate]\n",
      "   -------------------- ------------------- 1/2 [accelerate]\n",
      "   -------------------- ------------------- 1/2 [accelerate]\n",
      "   -------------------- ------------------- 1/2 [accelerate]\n",
      "   ---------------------------------------- 2/2 [accelerate]\n",
      "\n",
      "Successfully installed accelerate-1.12.0 sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets scipy numpy accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86794725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0351be5f9040dba47f3b808997949d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rohan KumarMishra\\miniconda3\\envs\\aml\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Rohan KumarMishra\\.cache\\huggingface\\hub\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93b78308c4846468e4818481f647260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline PPL: 14.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "m has more than 2 dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 210\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m window \u001b[38;5;129;01min\u001b[39;00m windows:\n\u001b[0;32m    204\u001b[0m     test_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    205\u001b[0m         MODEL_NAME,\n\u001b[0;32m    206\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mDTYPE,\n\u001b[0;32m    207\u001b[0m         device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m     )\n\u001b[1;32m--> 210\u001b[0m     perms \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_adjacent_permutations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_mats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m     merge_k_layers_llama(test_model, window, perms)\n\u001b[0;32m    213\u001b[0m     ppl \u001b[38;5;241m=\u001b[39m compute_perplexity(test_model, texts)\n",
      "Cell \u001b[1;32mIn[1], line 96\u001b[0m, in \u001b[0;36mcompute_adjacent_permutations\u001b[1;34m(feature_dict, window)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m window[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     95\u001b[0m     X_other \u001b[38;5;241m=\u001b[39m feature_dict[layer]\n\u001b[1;32m---> 96\u001b[0m     C \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrcoef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_anchor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_other\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowvar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     d \u001b[38;5;241m=\u001b[39m X_anchor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     98\u001b[0m     C \u001b[38;5;241m=\u001b[39m C[:d, d:]\n",
      "File \u001b[1;32mc:\\Users\\Rohan KumarMishra\\miniconda3\\envs\\aml\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3037\u001b[0m, in \u001b[0;36mcorrcoef\u001b[1;34m(x, y, rowvar, bias, ddof, dtype)\u001b[0m\n\u001b[0;32m   3033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;129;01mor\u001b[39;00m ddof \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue:\n\u001b[0;32m   3034\u001b[0m     \u001b[38;5;66;03m# 2015-03-15, 1.10\u001b[39;00m\n\u001b[0;32m   3035\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias and ddof have no effect and are deprecated\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   3036\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m-> 3037\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mcov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3038\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3039\u001b[0m     d \u001b[38;5;241m=\u001b[39m diag(c)\n",
      "File \u001b[1;32mc:\\Users\\Rohan KumarMishra\\miniconda3\\envs\\aml\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2807\u001b[0m, in \u001b[0;36mcov\u001b[1;34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[0m\n\u001b[0;32m   2805\u001b[0m m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(m)\n\u001b[0;32m   2806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 2807\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm has more than 2 dimensions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2810\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\n",
      "\u001b[1;31mValueError\u001b[0m: m has more than 2 dimensions"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LLaMA FFN Neuron Alignment + Weight-Tied Merging\n",
    "# Works for: LLaMA, TinyLlama, Mistral, Gemma, Qwen\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "MAX_TOKENS = 4000\n",
    "K = 2   # merge window size\n",
    "\n",
    "# -------------------------\n",
    "# Load model & tokenizer\n",
    "# -------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def get_blocks(model):\n",
    "    return model.model.layers\n",
    "\n",
    "def get_ffn(block):\n",
    "    return block.mlp  # gate_proj, up_proj, down_proj\n",
    "\n",
    "# -------------------------\n",
    "# FFN Feature Extraction (UP projection)\n",
    "# -------------------------\n",
    "def extract_ffn_features_single_pass(model, texts, max_tokens=MAX_TOKENS):\n",
    "    blocks = get_blocks(model)\n",
    "    features = {i: [] for i in range(len(blocks))}\n",
    "    hooks = []\n",
    "\n",
    "    def make_hook(i):\n",
    "        def hook(_, __, out):\n",
    "            features[i].append(out.detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    for i, block in enumerate(blocks):\n",
    "        hooks.append(\n",
    "            block.mlp.up_proj.register_forward_hook(make_hook(i))\n",
    "        )\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**enc)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    X = {}\n",
    "    for i in features:\n",
    "        acts = torch.cat(features[i], dim=0)\n",
    "        if acts.shape[0] > max_tokens:\n",
    "            acts = acts[:max_tokens]\n",
    "        X[i] = acts.numpy()\n",
    "\n",
    "    return X\n",
    "\n",
    "# -------------------------\n",
    "# Hungarian Neuron Matching\n",
    "# -------------------------\n",
    "def compute_adjacent_permutations(feature_dict, window):\n",
    "    anchor = window[0]\n",
    "    perms = {}\n",
    "\n",
    "    X_anchor = feature_dict[anchor]\n",
    "\n",
    "    for layer in window[1:]:\n",
    "        X_other = feature_dict[layer]\n",
    "        C = np.corrcoef(X_anchor, X_other, rowvar=False)\n",
    "        d = X_anchor.shape[1]\n",
    "        C = C[:d, d:]\n",
    "        _, col_ind = linear_sum_assignment(-C)\n",
    "        perms[layer] = col_ind\n",
    "\n",
    "    return perms\n",
    "\n",
    "# -------------------------\n",
    "# Sliding windows\n",
    "# -------------------------\n",
    "def sliding_windows(n_layers, k):\n",
    "    return [list(range(i, i + k)) for i in range(n_layers - k + 1)]\n",
    "\n",
    "# -------------------------\n",
    "# LLaMA FFN Merge (TRUE weight tying)\n",
    "# -------------------------\n",
    "def merge_k_layers_llama(model, layers, perms):\n",
    "    blocks = get_blocks(model)\n",
    "    anchor = layers[0]\n",
    "    anchor_mlp = blocks[anchor].mlp\n",
    "\n",
    "    device = anchor_mlp.up_proj.weight.device\n",
    "    k = len(layers)\n",
    "\n",
    "    W_gate = anchor_mlp.gate_proj.weight.data.clone()\n",
    "    W_up   = anchor_mlp.up_proj.weight.data.clone()\n",
    "    W_down = anchor_mlp.down_proj.weight.data.clone()\n",
    "\n",
    "    for layer in layers[1:]:\n",
    "        mlp = blocks[layer].mlp\n",
    "        perm = torch.tensor(perms[layer], device=device)\n",
    "\n",
    "        W_gate += mlp.gate_proj.weight[perm, :]\n",
    "        W_up   += mlp.up_proj.weight[perm, :]\n",
    "        W_down += mlp.down_proj.weight[:, perm]\n",
    "\n",
    "    W_gate /= k\n",
    "    W_up   /= k\n",
    "    W_down /= k\n",
    "\n",
    "    shared_gate = nn.Parameter(W_gate)\n",
    "    shared_up   = nn.Parameter(W_up)\n",
    "    shared_down = nn.Parameter(W_down)\n",
    "\n",
    "    for layer in layers:\n",
    "        mlp = blocks[layer].mlp\n",
    "        mlp.gate_proj.weight = shared_gate\n",
    "        mlp.up_proj.weight   = shared_up\n",
    "        mlp.down_proj.weight = shared_down\n",
    "\n",
    "# -------------------------\n",
    "# Perplexity\n",
    "# -------------------------\n",
    "def compute_perplexity(model, texts, batch_size=4):\n",
    "    model.eval()\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, :-1, :]\n",
    "            labels = input_ids[:, 1:]\n",
    "\n",
    "        mask = attention_mask[:, 1:].bool()\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = log_probs.gather(\n",
    "            dim=-1, index=labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        total_nll += -(token_log_probs[mask]).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    return math.exp(total_nll / total_tokens)\n",
    "\n",
    "# ============================================================\n",
    "# Main Experiment\n",
    "# ============================================================\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "texts = [t for t in dataset[\"text\"] if len(t.strip()) > 0][:200]\n",
    "\n",
    "base_ppl = compute_perplexity(model, texts)\n",
    "print(f\"Baseline PPL: {base_ppl:.2f}\")\n",
    "\n",
    "# Extract features ONCE\n",
    "feature_mats = extract_ffn_features_single_pass(model, texts)\n",
    "\n",
    "best_ppl = float(\"inf\")\n",
    "best_window = None\n",
    "best_state = None\n",
    "\n",
    "windows = sliding_windows(len(get_blocks(model)), K)\n",
    "\n",
    "for window in windows:\n",
    "    test_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    perms = compute_adjacent_permutations(feature_mats, window)\n",
    "    merge_k_layers_llama(test_model, window, perms)\n",
    "\n",
    "    ppl = compute_perplexity(test_model, texts)\n",
    "    print(f\"Window {window} | PPL {ppl:.2f}\")\n",
    "\n",
    "    if ppl < best_ppl:\n",
    "        best_ppl = ppl\n",
    "        best_window = window\n",
    "        best_state = {\n",
    "            k: v.detach().cpu().clone()\n",
    "            for k, v in test_model.state_dict().items()\n",
    "        }\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"BEST WINDOW: {best_window}\")\n",
    "print(f\"BEST PPL   : {best_ppl:.2f}\")\n",
    "print(\"==============================\")\n",
    "\n",
    "# -------------------------\n",
    "# Sanity: weight tying\n",
    "# -------------------------\n",
    "final_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "final_model.load_state_dict(best_state)\n",
    "\n",
    "l0 = final_model.model.layers[best_window[0]].mlp\n",
    "l1 = final_model.model.layers[best_window[1]].mlp\n",
    "\n",
    "print(\"Weight tied?\",\n",
    "      l0.up_proj.weight is l1.up_proj.weight,\n",
    "      l0.down_proj.weight is l1.down_proj.weight)\n",
    "\n",
    "# -------------------------\n",
    "# Generation demo\n",
    "# -------------------------\n",
    "prompt = \"India will become global leader in AI because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = final_model.generate(**inputs, max_length=40)\n",
    "\n",
    "print(\"\\nGenerated text:\")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d5ccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CPU model for feature extraction...\n",
      "Extracting FFN features (CPU)...\n",
      "Loading eval model...\n",
      "Baseline PPL: 14.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window [0, 1] | PPL 2306.26\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 239\u001b[0m\n\u001b[0;32m    237\u001b[0m         best_ppl \u001b[38;5;241m=\u001b[39m ppl\n\u001b[0;32m    238\u001b[0m         best_window \u001b[38;5;241m=\u001b[39m window\n\u001b[1;32m--> 239\u001b[0m         best_state \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    240\u001b[0m             k: v\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    241\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m test_model\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    242\u001b[0m         }\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==============================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBEST WINDOW: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_window\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 240\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    237\u001b[0m         best_ppl \u001b[38;5;241m=\u001b[39m ppl\n\u001b[0;32m    238\u001b[0m         best_window \u001b[38;5;241m=\u001b[39m window\n\u001b[0;32m    239\u001b[0m         best_state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 240\u001b[0m             k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    241\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m test_model\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    242\u001b[0m         }\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==============================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBEST WINDOW: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_window\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LLaMA / TinyLlama FFN Neuron Alignment + Weight-Tied Merging\n",
    "# FINAL DEVICE-SAFE VERSION\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MAX_TOKENS = 4000\n",
    "K = 2\n",
    "EVAL_TEXTS = 200\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "# -------------------------\n",
    "# Tokenizer\n",
    "# -------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------\n",
    "# Model loaders\n",
    "# -------------------------\n",
    "def load_model_cpu():\n",
    "    \"\"\"Used ONLY for feature extraction\"\"\"\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.float32,\n",
    "        device_map=None\n",
    "    ).eval()\n",
    "\n",
    "def load_model_eval():\n",
    "    \"\"\"Used for merging + perplexity\"\"\"\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=DTYPE,\n",
    "        device_map=\"auto\"\n",
    "    ).eval()\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def get_blocks(model):\n",
    "    return model.model.layers\n",
    "\n",
    "# -------------------------\n",
    "# FFN Feature Extraction (CPU ONLY)\n",
    "# -------------------------\n",
    "def extract_ffn_features_single_pass(model, texts, max_tokens=MAX_TOKENS):\n",
    "    blocks = get_blocks(model)\n",
    "    features = {i: [] for i in range(len(blocks))}\n",
    "    hooks = []\n",
    "\n",
    "    def make_hook(i):\n",
    "        def hook(_, __, out):\n",
    "            features[i].append(out.detach())\n",
    "        return hook\n",
    "\n",
    "    for i, block in enumerate(blocks):\n",
    "        hooks.append(\n",
    "            block.mlp.up_proj.register_forward_hook(make_hook(i))\n",
    "        )\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )  # ❗ stays on CPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**enc)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    X = {}\n",
    "    for i in features:\n",
    "        acts = torch.cat(features[i], dim=0)     # [B, T, d_ff]\n",
    "        acts = acts.reshape(-1, acts.shape[-1])  # [B*T, d_ff]\n",
    "\n",
    "        if acts.shape[0] > max_tokens:\n",
    "            acts = acts[:max_tokens]\n",
    "\n",
    "        X[i] = acts.numpy()\n",
    "\n",
    "    return X\n",
    "\n",
    "# -------------------------\n",
    "# Hungarian matching\n",
    "# -------------------------\n",
    "def compute_adjacent_permutations(feature_dict, window):\n",
    "    anchor = window[0]\n",
    "    perms = {}\n",
    "\n",
    "    Xa = feature_dict[anchor]\n",
    "\n",
    "    for layer in window[1:]:\n",
    "        Xb = feature_dict[layer]\n",
    "\n",
    "        C = np.corrcoef(Xa, Xb, rowvar=False)\n",
    "        d = Xa.shape[1]\n",
    "        C = C[:d, d:]\n",
    "\n",
    "        _, col_ind = linear_sum_assignment(-C)\n",
    "        perms[layer] = col_ind\n",
    "\n",
    "    return perms\n",
    "\n",
    "# -------------------------\n",
    "# Sliding windows\n",
    "# -------------------------\n",
    "def sliding_windows(n_layers, k):\n",
    "    return [list(range(i, i + k)) for i in range(n_layers - k + 1)]\n",
    "\n",
    "# -------------------------\n",
    "# LLaMA FFN merge (true tying)\n",
    "# -------------------------\n",
    "def merge_k_layers_llama(model, layers, perms):\n",
    "    blocks = get_blocks(model)\n",
    "    anchor = layers[0]\n",
    "    anchor_mlp = blocks[anchor].mlp\n",
    "\n",
    "    device = anchor_mlp.up_proj.weight.device\n",
    "    k = len(layers)\n",
    "\n",
    "    W_gate = anchor_mlp.gate_proj.weight.data.clone()\n",
    "    W_up   = anchor_mlp.up_proj.weight.data.clone()\n",
    "    W_down = anchor_mlp.down_proj.weight.data.clone()\n",
    "\n",
    "    for layer in layers[1:]:\n",
    "        mlp = blocks[layer].mlp\n",
    "        perm = torch.tensor(perms[layer], device=device)\n",
    "\n",
    "        W_gate += mlp.gate_proj.weight[perm, :]\n",
    "        W_up   += mlp.up_proj.weight[perm, :]\n",
    "        W_down += mlp.down_proj.weight[:, perm]\n",
    "\n",
    "    W_gate /= k\n",
    "    W_up   /= k\n",
    "    W_down /= k\n",
    "\n",
    "    shared_gate = nn.Parameter(W_gate)\n",
    "    shared_up   = nn.Parameter(W_up)\n",
    "    shared_down = nn.Parameter(W_down)\n",
    "\n",
    "    for layer in layers:\n",
    "        mlp = blocks[layer].mlp\n",
    "        mlp.gate_proj.weight = shared_gate\n",
    "        mlp.up_proj.weight   = shared_up\n",
    "        mlp.down_proj.weight = shared_down\n",
    "\n",
    "# -------------------------\n",
    "# Perplexity\n",
    "# -------------------------\n",
    "def compute_perplexity(model, texts, batch_size=4):\n",
    "    model.eval()\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, :-1, :]\n",
    "            labels = input_ids[:, 1:]\n",
    "\n",
    "        mask = attention_mask[:, 1:].bool()\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        token_log_probs = log_probs.gather(\n",
    "            dim=-1, index=labels.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        total_nll += -(token_log_probs[mask]).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    return math.exp(total_nll / total_tokens)\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "texts = [t for t in dataset[\"text\"] if len(t.strip()) > 0][:EVAL_TEXTS]\n",
    "\n",
    "print(\"Loading CPU model for feature extraction...\")\n",
    "cpu_model = load_model_cpu()\n",
    "\n",
    "print(\"Extracting FFN features (CPU)...\")\n",
    "feature_mats = extract_ffn_features_single_pass(cpu_model, texts)\n",
    "\n",
    "print(\"Loading eval model...\")\n",
    "base_model = load_model_eval()\n",
    "base_ppl = compute_perplexity(base_model, texts)\n",
    "print(f\"Baseline PPL: {base_ppl:.2f}\")\n",
    "\n",
    "best_ppl = float(\"inf\")\n",
    "best_window = None\n",
    "best_state = None\n",
    "\n",
    "windows = sliding_windows(len(get_blocks(base_model)), K)\n",
    "\n",
    "for window in windows:\n",
    "    test_model = load_model_eval()\n",
    "    perms = compute_adjacent_permutations(feature_mats, window)\n",
    "    merge_k_layers_llama(test_model, window, perms)\n",
    "\n",
    "    ppl = compute_perplexity(test_model, texts)\n",
    "    print(f\"Window {window} | PPL {ppl:.2f}\")\n",
    "\n",
    "    if ppl < best_ppl:\n",
    "        best_ppl = ppl\n",
    "        best_window = window\n",
    "        best_state = {\n",
    "            k: v.detach().cpu().clone()\n",
    "            for k, v in test_model.state_dict().items()\n",
    "        }\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"BEST WINDOW: {best_window}\")\n",
    "print(f\"BEST PPL   : {best_ppl:.2f}\")\n",
    "print(\"==============================\")\n",
    "\n",
    "# -------------------------\n",
    "# Sanity check: weight tying\n",
    "# -------------------------\n",
    "final_model = load_model_eval()\n",
    "final_model.load_state_dict(best_state)\n",
    "\n",
    "l0 = final_model.model.layers[best_window[0]].mlp\n",
    "l1 = final_model.model.layers[best_window[1]].mlp\n",
    "\n",
    "print(\"Weights tied?\",\n",
    "      l0.up_proj.weight is l1.up_proj.weight,\n",
    "      l0.down_proj.weight is l1.down_proj.weight)\n",
    "\n",
    "# -------------------------\n",
    "# Generation demo\n",
    "# -------------------------\n",
    "prompt = \"India will become global leader in AI because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = final_model.generate(**inputs, max_length=40)\n",
    "\n",
    "print(\"\\nGenerated text:\")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b8d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
