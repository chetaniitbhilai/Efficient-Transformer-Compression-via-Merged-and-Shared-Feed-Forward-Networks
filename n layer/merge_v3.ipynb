{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa87730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading base model...\n",
      "Extracting FFN features...\n",
      "\n",
      "Searching best window...\n",
      "Window [0, 1, 2, 3] | PPL 3260.79\n",
      "Window [1, 2, 3, 4] | PPL 395.99\n",
      "Window [2, 3, 4, 5] | PPL 128.06\n",
      "Window [3, 4, 5, 6] | PPL 102.01\n",
      "Window [4, 5, 6, 7] | PPL 92.25\n",
      "Window [5, 6, 7, 8] | PPL 91.09\n",
      "Window [6, 7, 8, 9] | PPL 92.88\n",
      "Window [7, 8, 9, 10] | PPL 98.46\n",
      "Window [8, 9, 10, 11] | PPL 350.21\n",
      "\n",
      "Best window: [5, 6, 7, 8]\n",
      "\n",
      "Starting fine-tuning...\n",
      "Step 0 | Loss 6.0397\n",
      "Step 50 | Loss 4.5996\n",
      "Step 100 | Loss 4.4414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Validation PPL: 50.86\n",
      "\n",
      "Generated text:\n",
      "India will become global leader in AI because of the fact of its ability to communicate with the human.\n",
      "\n",
      "The AI is a global leader in the world's AI. It is the most advanced AI\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Soft Shared FFN (Wc + Low-Rank Delta) for GPT-2\n",
    "# No averaging, no MoE, no information loss\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RANK = 8\n",
    "WINDOW_K = 4\n",
    "LAMBDA = 1e-4\n",
    "MAX_TOKENS = 4000\n",
    "EVAL_TEXTS = 200\n",
    "\n",
    "# -------------------------\n",
    "# Tokenizer\n",
    "# -------------------------\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ============================================================\n",
    "# Low-rank FFN module\n",
    "# ============================================================\n",
    "\n",
    "class LowRankLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 compatible low-rank linear:\n",
    "    weight layout = [in_dim, out_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, Wc, bias, rank):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Wc = Wc          # shared base weight\n",
    "        self.bias = bias\n",
    "\n",
    "        in_dim, out_dim = Wc.shape   # ðŸ”‘ GPT-2 layout\n",
    "\n",
    "        device = Wc.device\n",
    "\n",
    "        # Low-rank delta in SAME layout as Wc\n",
    "        self.A = nn.Parameter(torch.zeros(in_dim, rank, device=device))\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim, device=device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        W_eff = self.Wc + self.A @ self.B   # shape [in_dim, out_dim]\n",
    "        return F.linear(x, W_eff.T, self.bias)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# ============================================================\n",
    "# Feature extraction (c_fc activations)\n",
    "# ============================================================\n",
    "\n",
    "def extract_ffn_features_single_pass(model, texts):\n",
    "    model.eval()\n",
    "    features = {i: [] for i in range(len(model.transformer.h))}\n",
    "    hooks = []\n",
    "\n",
    "    def make_hook(i):\n",
    "        def hook(_, __, out):\n",
    "            features[i].append(out.detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    for i, block in enumerate(model.transformer.h):\n",
    "        hooks.append(block.mlp.c_fc.register_forward_hook(make_hook(i)))\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**enc)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    X = {}\n",
    "    for i in features:\n",
    "        acts = torch.cat(features[i], dim=0)\n",
    "        acts = acts.reshape(-1, acts.shape[-1])\n",
    "        X[i] = acts[:MAX_TOKENS].numpy()\n",
    "\n",
    "    return X\n",
    "\n",
    "# ============================================================\n",
    "# Hungarian alignment\n",
    "# ============================================================\n",
    "\n",
    "def compute_adjacent_permutations(feature_dict, window):\n",
    "    anchor = window[0]\n",
    "    perms = {}\n",
    "    Xa = feature_dict[anchor]\n",
    "\n",
    "    for layer in window[1:]:\n",
    "        Xb = feature_dict[layer]\n",
    "        C = np.corrcoef(Xa, Xb, rowvar=False)\n",
    "        d = Xa.shape[1]\n",
    "        C = C[:d, d:]\n",
    "        _, col = linear_sum_assignment(-C)\n",
    "        perms[layer] = col\n",
    "\n",
    "    return perms\n",
    "\n",
    "def sliding_windows(n, k):\n",
    "    return [list(range(i, i + k)) for i in range(n - k + 1)]\n",
    "\n",
    "# ============================================================\n",
    "# Replace FFNs with soft shared base + low-rank delta\n",
    "# ============================================================\n",
    "\n",
    "def soft_share_ffn_lowrank(model, layers, perms, rank=RANK):\n",
    "    anchor = layers[0]\n",
    "    anchor_mlp = model.transformer.h[anchor].mlp\n",
    "\n",
    "    device = anchor_mlp.c_fc.weight.device\n",
    "\n",
    "    Wc_fc = nn.Parameter(anchor_mlp.c_fc.weight.data.clone().to(device))\n",
    "    bc_fc = nn.Parameter(anchor_mlp.c_fc.bias.data.clone().to(device))\n",
    "    Wc_proj = nn.Parameter(anchor_mlp.c_proj.weight.data.clone().to(device))\n",
    "    bc_proj = nn.Parameter(anchor_mlp.c_proj.bias.data.clone().to(device))\n",
    "\n",
    "\n",
    "    for layer in layers:\n",
    "        mlp = model.transformer.h[layer].mlp\n",
    "        perm = perms.get(layer, None)\n",
    "\n",
    "        # c_fc\n",
    "        mlp.c_fc = LowRankLinear(\n",
    "        Wc=Wc_fc,\n",
    "        bias=bc_fc,\n",
    "        rank=rank\n",
    "        )\n",
    "\n",
    "        mlp.c_proj = LowRankLinear(\n",
    "        Wc=Wc_proj,\n",
    "        bias=bc_proj,\n",
    "        rank=rank\n",
    "        )\n",
    "\n",
    "# ============================================================\n",
    "# Loss helpers\n",
    "# ============================================================\n",
    "\n",
    "def lm_loss(model, batch):\n",
    "    input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits[:, :-1, :]\n",
    "    labels = input_ids[:, 1:]\n",
    "\n",
    "    mask = attention_mask[:, 1:].bool()\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_log_probs = log_probs.gather(\n",
    "        dim=-1, index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    return -(token_log_probs[mask]).mean()\n",
    "\n",
    "def consensus_loss(model, layers, lam=LAMBDA):\n",
    "    loss = 0.0\n",
    "    for l in layers:\n",
    "        mlp = model.transformer.h[l].mlp\n",
    "        loss += torch.norm(mlp.c_fc.A @ mlp.c_fc.B, p=\"fro\")**2\n",
    "        loss += torch.norm(mlp.c_proj.A @ mlp.c_proj.B, p=\"fro\")**2\n",
    "    return lam * loss\n",
    "\n",
    "# ============================================================\n",
    "# Perplexity\n",
    "# ============================================================\n",
    "\n",
    "def compute_perplexity(model, texts, batch_size=4):\n",
    "    model.eval()\n",
    "    total_nll = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        enc = tokenizer(\n",
    "            texts[i:i + batch_size],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(enc[\"input_ids\"]).logits[:, :-1, :]\n",
    "            labels = enc[\"input_ids\"][:, 1:]\n",
    "            mask = enc[\"attention_mask\"][:, 1:].bool()\n",
    "\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            lp = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        total_nll += -(lp[mask]).sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    return math.exp(total_nll / total_tokens)\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading data...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "texts = [t for t in dataset[\"text\"] if len(t.strip()) > 0][:EVAL_TEXTS]\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "\n",
    "print(\"Extracting FFN features...\")\n",
    "features = extract_ffn_features_single_pass(base_model, texts)\n",
    "\n",
    "windows = sliding_windows(len(base_model.transformer.h), WINDOW_K)\n",
    "\n",
    "best_ppl = float(\"inf\")\n",
    "best_window = None\n",
    "\n",
    "print(\"\\nSearching best window...\")\n",
    "for w in windows:\n",
    "    test_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "    perms = compute_adjacent_permutations(features, w)\n",
    "    soft_share_ffn_lowrank(test_model, w, perms)\n",
    "\n",
    "    ppl = compute_perplexity(test_model, texts)\n",
    "    print(f\"Window {w} | PPL {ppl:.2f}\")\n",
    "\n",
    "    if ppl < best_ppl:\n",
    "        best_ppl = ppl\n",
    "        best_window = w\n",
    "\n",
    "print(\"\\nBest window:\", best_window)\n",
    "\n",
    "# ============================================================\n",
    "# Fine-tuning\n",
    "# ============================================================\n",
    "\n",
    "train_ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:2%]\")\n",
    "train_texts = [t for t in train_ds[\"text\"] if len(t.strip()) > 0]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "train_loader = DataLoader(train_texts, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(DEVICE)\n",
    "perms = compute_adjacent_permutations(features, best_window)\n",
    "soft_share_ffn_lowrank(model, best_window, perms)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "print(\"\\nStarting fine-tuning...\")\n",
    "model.train()\n",
    "for step, batch in enumerate(train_loader):\n",
    "    if step >= 300:\n",
    "        break\n",
    "\n",
    "    loss = lm_loss(model, batch)\n",
    "    loss += consensus_loss(model, best_window)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step} | Loss {loss.item():.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# Final evaluation\n",
    "# ============================================================\n",
    "\n",
    "val_ppl = compute_perplexity(model, texts)\n",
    "print(f\"\\nFinal Validation PPL: {val_ppl:.2f}\")\n",
    "\n",
    "prompt = \"India will become global leader in AI because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_length=40)\n",
    "\n",
    "print(\"\\nGenerated text:\")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0392f7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST PPL after recovery: 68.58\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\n",
    "    \"wikitext\", \"wikitext-2-raw-v1\", split=\"test\"\n",
    ")\n",
    "\n",
    "test_texts = [t for t in test_dataset[\"text\"] if len(t.strip()) > 0][:500]\n",
    "test_ppl = compute_perplexity(model, test_texts)\n",
    "print(f\"TEST PPL after recovery: {test_ppl:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
