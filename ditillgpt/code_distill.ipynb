{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3d6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")  # ~82M params\n",
    "tok = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97255f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.912576 81.912576\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(total/1e6, trainable/1e6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "472e0ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18359' max='18359' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18359/18359 13:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.462100</td>\n",
       "      <td>3.561412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [470/470 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 3.5614\n",
      "Perplexity: 35.21\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "# Load (your pruned model OR distilgpt2 if debugging)\n",
    "# model = your_pruned_model\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# tok = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tok.pad_token = tok.eos_token  # important for padding\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "def preprocess(ex):\n",
    "    return tok(ex[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tok_ds = ds.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# âœ… THIS is what fixes your error\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tok,\n",
    "    mlm=False  # because GPT2 is causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Training args\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-run\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tok_ds[\"train\"],\n",
    "    eval_dataset=tok_ds[\"validation\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Perplexity\n",
    "res = trainer.evaluate()\n",
    "print(f\"Eval Loss: {res['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity: {math.exp(res['eval_loss']):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdd3e135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 212762.38 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:00<00:00, 199172.97 examples/s]\n",
      "Generating unsupervised split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:00<00:00, 274822.96 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:26<00:00, 943.74 examples/s] \n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:25<00:00, 981.94 examples/s] \n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:50<00:00, 982.97 examples/s] \n",
      "Downloading builder script: 4.20kB [00:00, 5.75MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Training DistilGPT2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 05:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.819400</td>\n",
       "      <td>0.660719</td>\n",
       "      <td>0.860200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DistilGPT2 Accuracy: 0.8602\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, GPT2ForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --------------------------\n",
    "# 1. Load IMDb Dataset\n",
    "# --------------------------\n",
    "ds = load_dataset(\"imdb\")\n",
    "ds = ds.shuffle(seed=42)\n",
    "\n",
    "# --------------------------\n",
    "# 2. Tokenizer (GPT-2 / DistilGPT2)\n",
    "# --------------------------\n",
    "tok = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tok(\n",
    "        batch[\"text\"], truncation=True, padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "ds_tok = ds.map(tokenize, batched=True)\n",
    "ds_tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "train_ds = ds_tok[\"train\"].select(range(20000))   # smaller subset for speed\n",
    "test_ds  = ds_tok[\"test\"].select(range(5000))\n",
    "\n",
    "# --------------------------\n",
    "# 3. Load Distilled GPT-2 for Classification\n",
    "# --------------------------\n",
    "def load_distil_model():\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(\n",
    "        \"distilgpt2\",\n",
    "        num_labels=2,\n",
    "        pad_token_id=tok.eos_token_id\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# --------------------------\n",
    "# 4. Metrics\n",
    "# --------------------------\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return acc.compute(predictions=preds, references=labels)\n",
    "\n",
    "# --------------------------\n",
    "# 5. Train & Evaluate DistilGPT2\n",
    "# --------------------------\n",
    "print(\"\\nðŸ”¹ Training DistilGPT2 ...\")\n",
    "model = load_distil_model()\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results_distilgpt2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=200\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "result = trainer.evaluate()\n",
    "print(f\"âœ… DistilGPT2 Accuracy:\", result[\"eval_accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7c7ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: cfcbe770-7794-45f5-8e56-5a821a608e80)')' thrown while requesting HEAD https://huggingface.co/datasets/cnn_dailymail/resolve/main/README.md\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 287113/287113 [00:03<00:00, 92212.53 examples/s] \n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13368/13368 [00:00<00:00, 92840.06 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11490/11490 [00:00<00:00, 91882.50 examples/s]\n",
      "Downloading builder script: 6.27kB [00:00, 5.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Evaluating ROUGE for: distilgpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 1/100 [00:00<00:37,  2.64it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  2%|â–         | 2/100 [00:00<00:27,  3.60it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  3%|â–Ž         | 3/100 [00:00<00:23,  4.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|â–         | 4/100 [00:00<00:22,  4.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  5%|â–Œ         | 5/100 [00:01<00:20,  4.53it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  6%|â–Œ         | 6/100 [00:01<00:20,  4.62it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  7%|â–‹         | 7/100 [00:01<00:19,  4.67it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|â–Š         | 8/100 [00:01<00:19,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  9%|â–‰         | 9/100 [00:02<00:19,  4.77it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 10%|â–ˆ         | 10/100 [00:02<00:18,  4.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 11%|â–ˆ         | 11/100 [00:02<00:16,  5.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|â–ˆâ–        | 12/100 [00:02<00:16,  5.19it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 13%|â–ˆâ–Ž        | 13/100 [00:02<00:17,  5.08it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 14%|â–ˆâ–        | 14/100 [00:02<00:17,  4.97it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 15%|â–ˆâ–Œ        | 15/100 [00:03<00:17,  4.95it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 16%|â–ˆâ–Œ        | 16/100 [00:03<00:17,  4.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|â–ˆâ–‹        | 17/100 [00:03<00:17,  4.87it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 18%|â–ˆâ–Š        | 18/100 [00:03<00:16,  4.83it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 19%|â–ˆâ–‰        | 19/100 [00:04<00:16,  4.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|â–ˆâ–ˆ        | 20/100 [00:04<00:16,  4.84it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|â–ˆâ–ˆ        | 21/100 [00:04<00:16,  4.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 22%|â–ˆâ–ˆâ–       | 22/100 [00:04<00:16,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:04<00:16,  4.78it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 24%|â–ˆâ–ˆâ–       | 24/100 [00:05<00:15,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:05<00:15,  4.83it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:05<00:15,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 27%|â–ˆâ–ˆâ–‹       | 27/100 [00:05<00:15,  4.84it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 28%|â–ˆâ–ˆâ–Š       | 28/100 [00:05<00:14,  4.85it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|â–ˆâ–ˆâ–‰       | 29/100 [00:06<00:14,  4.85it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:06<00:14,  4.85it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:06<00:14,  4.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:14,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:06<00:14,  4.74it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:13,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:07<00:13,  4.83it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:07<00:13,  4.83it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:07<00:13,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:07<00:12,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:08<00:12,  4.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:08<00:12,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:08<00:12,  4.83it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:08<00:12,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:09<00:11,  4.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:09<00:11,  4.85it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:09<00:11,  4.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:09<00:11,  4.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:09<00:10,  4.90it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:10<00:10,  4.87it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:10<00:10,  4.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:10<00:10,  4.90it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:10<00:10,  4.88it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:10<00:09,  4.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:11<00:09,  4.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:11<00:09,  4.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:11<00:09,  4.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:11<00:09,  4.88it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:11<00:08,  4.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:12<00:08,  4.88it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:12<00:08,  4.84it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:12<00:08,  4.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:12<00:07,  4.88it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:12<00:07,  4.85it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:13<00:07,  4.88it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:13<00:07,  4.91it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:13<00:06,  5.30it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:13<00:06,  5.19it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:13<00:06,  5.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:14<00:06,  5.03it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:14<00:06,  5.00it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:14<00:06,  4.98it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:14<00:05,  4.97it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:14<00:05,  5.10it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:15<00:05,  5.03it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:15<00:05,  4.95it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:15<00:05,  4.95it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:15<00:04,  4.91it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:15<00:04,  4.91it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:16<00:04,  4.90it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:16<00:04,  4.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:16<00:04,  4.90it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:16<00:03,  4.87it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:16<00:03,  4.87it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:17<00:03,  4.88it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:17<00:03,  4.88it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:17<00:03,  4.90it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:17<00:02,  4.88it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:17<00:02,  4.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:18<00:02,  4.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:18<00:02,  4.97it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:18<00:02,  4.95it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:18<00:01,  4.95it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:18<00:01,  4.93it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:19<00:01,  4.91it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:19<00:01,  4.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:19<00:01,  4.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:19<00:00,  4.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:19<00:00,  4.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [00:20<00:00,  5.17it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:20<00:00,  5.05it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:20<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ROUGE for distilgpt2: {'rouge1': np.float64(0.22278047339844204), 'rouge2': np.float64(0.09867662047952458), 'rougeL': np.float64(0.16022128844320674), 'rougeLsum': np.float64(0.188834834282039)}\n",
      "\n",
      "ðŸ“Š Final ROUGE-L Comparison:\n",
      "distilgpt2 â†’ ROUGE-L: 0.16022128844320674\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --------------------------\n",
    "# 1. Dataset: CNN/DailyMail (first 100 test samples)\n",
    "# --------------------------\n",
    "data = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:100]\")  \n",
    "\n",
    "# --------------------------\n",
    "# 2. ROUGE Metric\n",
    "# --------------------------\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# --------------------------\n",
    "# 3. Tokenizer (same for all GPT-based models)\n",
    "# --------------------------\n",
    "tok = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "# --------------------------\n",
    "# 4. Models to Evaluate\n",
    "# --------------------------\n",
    "models = {\n",
    "    \"distilgpt2\": \"distilgpt2\"   # âœ… added distilled model\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# 5. Summarization Function\n",
    "# --------------------------\n",
    "def generate_summary(model, text, max_new_tokens=60):\n",
    "    inp = tok(\n",
    "        text, return_tensors=\"pt\",\n",
    "        truncation=True, padding=True, max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=3,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    return tok.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# --------------------------\n",
    "# 6. Evaluation Loop\n",
    "# --------------------------\n",
    "results = {}\n",
    "\n",
    "for name, path in models.items():\n",
    "    print(f\"\\nðŸ”¹ Evaluating ROUGE for: {name}\")\n",
    "    \n",
    "    model = GPT2LMHeadModel.from_pretrained(path).to(device).eval()\n",
    "    predictions, references = [], []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        article = data[i][\"article\"]\n",
    "        ref = data[i][\"highlights\"]\n",
    "        pred = generate_summary(model, article)\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "\n",
    "    score = rouge.compute(predictions=predictions, references=references)\n",
    "    results[name] = score\n",
    "    print(f\"âœ… ROUGE for {name}:\", score)\n",
    "\n",
    "# --------------------------\n",
    "# 7. Final Comparison\n",
    "# --------------------------\n",
    "print(\"\\nðŸ“Š Final ROUGE-L Comparison:\")\n",
    "for model_name, score in results.items():\n",
    "    print(model_name, \"â†’ ROUGE-L:\", score[\"rougeL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130c234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myexamenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
