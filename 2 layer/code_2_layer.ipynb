{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b85aec27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: load tokenizer/model\n",
      "Step 1: sample text from WikiText-2\n",
      "Step 2: collect FFN pre-activations\n",
      "âœ“ feature matrices built\n",
      "Pairs to merge: [(0, 1), (2, 3), (4, 5), (6, 7), (8, 9), (10, 11)]\n",
      "Step 3: align each src to its dst (anchor = smaller index in pair)\n",
      "  L0â†”L1: mean aligned corr=0.3734\n",
      "  L2â†”L3: mean aligned corr=0.5047\n",
      "  L4â†”L5: mean aligned corr=0.5289\n",
      "  L6â†”L7: mean aligned corr=0.5190\n",
      "  L8â†”L9: mean aligned corr=0.5035\n",
      "  L10â†”L11: mean aligned corr=0.5042\n",
      "Step 4: apply permutations for each src, then merge FFNs into dst\n",
      "  merged FFN of L1 into L0  | layout=transposed, d_ff=3072, d_model=768\n",
      "  merged FFN of L3 into L2  | layout=transposed, d_ff=3072, d_model=768\n",
      "  merged FFN of L5 into L4  | layout=transposed, d_ff=3072, d_model=768\n",
      "  merged FFN of L7 into L6  | layout=transposed, d_ff=3072, d_model=768\n",
      "  merged FFN of L9 into L8  | layout=transposed, d_ff=3072, d_model=768\n",
      "  merged FFN of L11 into L10  | layout=transposed, d_ff=3072, d_model=768\n",
      "Step 5: build a compressed GPT-2 with half the layers, copying anchors\n",
      "Step 6: parameter counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params - original:   124,439,808\n",
      "Params - merged_full (same depth): 124,439,808  (should match original; merging alone doesnâ€™t cut params)\n",
      "Params - compressed_half (half depth): 81,912,576  (reduced)\n",
      "\n",
      "Original:\n",
      " The future of artificial intelligence in India is uncertain. The Indian government has been trying to develop a new AI system that can be used to solve problems in the field of AI. The Indian government has been\n",
      "\n",
      "Compressed (half layers):\n",
      " The future of artificial intelligence in India lasted forever\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, time, math, numpy as np, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# ------------------------------\n",
    "# Utils\n",
    "# ------------------------------\n",
    "\n",
    "def corr_matrix_fast(A, B):\n",
    "    A0 = A - A.mean(axis=0, keepdims=True)\n",
    "    B0 = B - B.mean(axis=0, keepdims=True)\n",
    "    As = A0.std(axis=0, ddof=1, keepdims=True); As[As==0]=1.0\n",
    "    Bs = B0.std(axis=0, ddof=1, keepdims=True); Bs[Bs==0]=1.0\n",
    "    Az = A0/As; Bz = B0/Bs\n",
    "    return (Az.T @ Bz) / (A.shape[0]-1)\n",
    "\n",
    "def detect_ffn_layout(block):\n",
    "    s1 = tuple(block.mlp.c_fc.weight.shape)\n",
    "    s2 = tuple(block.mlp.c_proj.weight.shape)\n",
    "    # Two possibilities for GPT2-base:\n",
    "    # standard: c_fc=[d_ff, d_model], c_proj=[d_model, d_ff]\n",
    "    # transposed: c_fc=[d_model, d_ff], c_proj=[d_ff, d_model]\n",
    "    d1, d2 = s1\n",
    "    if d1 > d2 and s2 == (d2, d1):\n",
    "        return \"standard\", d1, d2  # d_ff, d_model\n",
    "    elif d2 > d1 and s2 == (d2, d1):\n",
    "        return \"transposed\", d2, d1 # d_ff, d_model\n",
    "    else:\n",
    "        # fall back by comparing which dim matches embed size\n",
    "        d_model_guess = block.ln_2.weight.shape[0]\n",
    "        if s1[0] == d_model_guess:\n",
    "            return \"transposed\", s1[1], s1[0]\n",
    "        else:\n",
    "            return \"standard\", s1[0], s1[1]\n",
    "\n",
    "def hook_pre_ffn(block_id, store):\n",
    "    def _hook(module, inp, out):\n",
    "        store[block_id].append(out.detach().cpu())\n",
    "    return _hook\n",
    "\n",
    "def build_feature_mats(model, tokenizer, texts, max_len=128, max_rows=4000):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    # storage\n",
    "    store = {i: [] for i in range(len(model.h))}\n",
    "    hooks = []\n",
    "    for i, block in enumerate(model.h):\n",
    "        hooks.append(block.mlp.c_fc.register_forward_hook(hook_pre_ffn(i, store)))\n",
    "    # tokenize\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    enc = {k:v.to(device) for k,v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        _ = model(**enc)\n",
    "    for h in hooks: h.remove()\n",
    "\n",
    "    feats = {}\n",
    "    for i in range(len(model.h)):\n",
    "        X = torch.cat(store[i], dim=0)  # [B, T, d_ff]\n",
    "        X = X.reshape(-1, X.shape[-1])  # [N, d_ff]\n",
    "        if X.shape[0] > max_rows:\n",
    "            X = X[:max_rows]\n",
    "        feats[i] = X.numpy()\n",
    "    return feats\n",
    "\n",
    "def align_layers(Xa, Xb):\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    C = corr_matrix_fast(Xa, Xb)  # [d,d]\n",
    "    r, c = linear_sum_assignment(-C)\n",
    "    return c, float(C[r, c].mean()), C\n",
    "\n",
    "# Apply permutation to **one** block, handling both layouts.\n",
    "def apply_ffn_permutation_block(model, layer_idx, perm):\n",
    "    blk = model.h[layer_idx].mlp\n",
    "    layout, d_ff, d_model = detect_ffn_layout(model.h[layer_idx])\n",
    "    p = torch.as_tensor(perm, dtype=torch.long, device=blk.c_fc.weight.device)\n",
    "    with torch.no_grad():\n",
    "        if layout == \"standard\":\n",
    "            # c_fc: [d_ff, d_model] rows; c_proj: [d_model, d_ff] cols\n",
    "            blk.c_fc.weight[:]  = blk.c_fc.weight[p, :]\n",
    "            blk.c_fc.bias[:]    = blk.c_fc.bias[p]\n",
    "            blk.c_proj.weight[:] = blk.c_proj.weight[:, p]\n",
    "        else:\n",
    "            # transposed: c_fc: [d_model, d_ff] cols; c_proj: [d_ff, d_model] rows\n",
    "            blk.c_fc.weight[:]   = blk.c_fc.weight[:, p]\n",
    "            blk.c_fc.bias[:]     = blk.c_fc.bias[p]\n",
    "            blk.c_proj.weight[:] = blk.c_proj.weight[p, :]\n",
    "    return layout, d_ff, d_model\n",
    "\n",
    "# Merge FFN of 'src_idx' into 'dst_idx' (weights already aligned by permutation).\n",
    "# Simple average merge: W1, b1, W2, b2.\n",
    "def merge_ffn_pair(model, dst_idx, src_idx):\n",
    "    dst = model.h[dst_idx].mlp\n",
    "    src = model.h[src_idx].mlp\n",
    "    with torch.no_grad():\n",
    "        dst.c_fc.weight[:]   = 0.5 * (dst.c_fc.weight + src.c_fc.weight)\n",
    "        dst.c_fc.bias[:]     = 0.5 * (dst.c_fc.bias   + src.c_fc.bias)\n",
    "        dst.c_proj.weight[:] = 0.5 * (dst.c_proj.weight + src.c_proj.weight)\n",
    "        dst.c_proj.bias[:]   = 0.5 * (dst.c_proj.bias   + src.c_proj.bias)\n",
    "\n",
    "# Build a compressed GPT-2 (half the layers) and copy weights from anchors.\n",
    "def build_compressed_gpt2(original, keep_layers):\n",
    "    device = next(original.parameters()).device\n",
    "    cfg = original.config\n",
    "    new_cfg = GPT2Config(\n",
    "        vocab_size=cfg.vocab_size,\n",
    "        n_positions=cfg.n_positions,\n",
    "        n_ctx=cfg.n_ctx,\n",
    "        n_embd=cfg.n_embd,\n",
    "        n_layer=len(keep_layers),\n",
    "        n_head=cfg.n_head,\n",
    "        n_inner=cfg.n_inner,\n",
    "        activation_function=cfg.activation_function,\n",
    "        resid_pdrop=cfg.resid_pdrop,\n",
    "        embd_pdrop=cfg.embd_pdrop,\n",
    "        attn_pdrop=cfg.attn_pdrop,\n",
    "        layer_norm_epsilon=cfg.layer_norm_epsilon,\n",
    "        initializer_range=cfg.initializer_range,\n",
    "        bos_token_id=getattr(cfg, \"bos_token_id\", None),\n",
    "        eos_token_id=getattr(cfg, \"eos_token_id\", None),\n",
    "    )\n",
    "    tiny = GPT2LMHeadModel(new_cfg).to(device)\n",
    "    # embeddings / ln_f / lm_head\n",
    "    with torch.no_grad():\n",
    "        tiny.transformer.wte.weight[:] = original.transformer.wte.weight\n",
    "        tiny.transformer.wpe.weight[:] = original.transformer.wpe.weight[:tiny.transformer.wpe.weight.shape[0]]\n",
    "        tiny.transformer.ln_f.weight[:] = original.transformer.ln_f.weight\n",
    "        tiny.transformer.ln_f.bias[:]   = original.transformer.ln_f.bias\n",
    "        tiny.lm_head.weight[:] = original.lm_head.weight\n",
    "\n",
    "        # copy blocks\n",
    "        for new_i, old_i in enumerate(keep_layers):\n",
    "            src = original.transformer.h[old_i]\n",
    "            dst = tiny.transformer.h[new_i]\n",
    "            # attn weights/biases\n",
    "            for name in [\"ln_1\",\"attn\",\"ln_2\",\"mlp\"]:\n",
    "                # ln weights/bias\n",
    "                if name==\"ln_1\":\n",
    "                    dst.ln_1.weight[:] = src.ln_1.weight; dst.ln_1.bias[:] = src.ln_1.bias\n",
    "                elif name==\"ln_2\":\n",
    "                    dst.ln_2.weight[:] = src.ln_2.weight; dst.ln_2.bias[:] = src.ln_2.bias\n",
    "                elif name==\"attn\":\n",
    "                    # qkv + proj\n",
    "                    dst.attn.c_attn.weight[:] = src.attn.c_attn.weight\n",
    "                    dst.attn.c_attn.bias[:]   = src.attn.c_attn.bias\n",
    "                    dst.attn.c_proj.weight[:] = src.attn.c_proj.weight\n",
    "                    dst.attn.c_proj.bias[:]   = src.attn.c_proj.bias\n",
    "                elif name==\"mlp\":\n",
    "                    dst.mlp.c_fc.weight[:]   = src.mlp.c_fc.weight\n",
    "                    dst.mlp.c_fc.bias[:]     = src.mlp.c_fc.bias\n",
    "                    dst.mlp.c_proj.weight[:] = src.mlp.c_proj.weight\n",
    "                    dst.mlp.c_proj.bias[:]   = src.mlp.c_proj.bias\n",
    "    return tiny\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# ------------------------------\n",
    "# Pipeline\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    torch.set_grad_enabled(False)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    print(\"Step 0: load tokenizer/model\")\n",
    "    tok = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tok.pad_token = tok.eos_token\n",
    "    base = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device).eval()\n",
    "    base.config.pad_token_id = tok.eos_token_id\n",
    "\n",
    "    print(\"Step 1: sample text from WikiText-2\")\n",
    "    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:1%]\")\n",
    "    texts = [t for t in ds[\"text\"] if len(t.strip())>0][:64]\n",
    "\n",
    "    print(\"Step 2: collect FFN pre-activations\")\n",
    "    feats = build_feature_mats(base.transformer, tok, texts, max_len=96, max_rows=4000)\n",
    "    print(\"âœ“ feature matrices built\")\n",
    "\n",
    "    # We'll merge pairs: (0,1), (2,3), ...\n",
    "    L = len(base.transformer.h)\n",
    "    pairs = [(i, i+1) for i in range(0, L, 2) if i+1 < L]\n",
    "    print(\"Pairs to merge:\", pairs)\n",
    "\n",
    "    from collections import defaultdict\n",
    "    perms = {}\n",
    "\n",
    "    print(\"Step 3: align each src to its dst (anchor = smaller index in pair)\")\n",
    "    for dst, src in pairs:\n",
    "        Xa = feats[dst]; Xb = feats[src]\n",
    "        perm, score, _ = align_layers(Xa, Xb)\n",
    "        perms[(dst,src)] = perm\n",
    "        print(f\"  L{dst}â†”L{src}: mean aligned corr={score:.4f}\")\n",
    "\n",
    "    print(\"Step 4: apply permutations for each src, then merge FFNs into dst\")\n",
    "    for dst, src in pairs:\n",
    "        # apply permutation to **source** block to match dst ordering\n",
    "        layout, d_ff, d_model = apply_ffn_permutation_block(base.transformer, src, perms[(dst,src)])\n",
    "        # merge (average) src into dst FFN\n",
    "        merge_ffn_pair(base.transformer, dst, src)\n",
    "        print(f\"  merged FFN of L{src} into L{dst}  | layout={layout}, d_ff={d_ff}, d_model={d_model}\")\n",
    "\n",
    "    print(\"Step 5: build a compressed GPT-2 with half the layers, copying anchors\")\n",
    "    keep_layers = [i for i in range(0, L, 2)]  # keep 0,2,4,...\n",
    "    tiny = build_compressed_gpt2(base, keep_layers).eval()\n",
    "\n",
    "    # Save models\n",
    "    os.makedirs(\"gpt2_merged_full\", exist_ok=True)\n",
    "    os.makedirs(\"gpt2_compressed_half\", exist_ok=True)\n",
    "    base.save_pretrained(\"gpt2_merged_full\")\n",
    "    tok.save_pretrained(\"gpt2_merged_full\")\n",
    "    tiny.save_pretrained(\"gpt2_compressed_half\")\n",
    "    tok.save_pretrained(\"gpt2_compressed_half\")\n",
    "\n",
    "    print(\"Step 6: parameter counts\")\n",
    "    orig = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device).eval()\n",
    "    merged_full = GPT2LMHeadModel.from_pretrained(\"gpt2_merged_full\").to(device).eval()\n",
    "    compressed = GPT2LMHeadModel.from_pretrained(\"gpt2_compressed_half\").to(device).eval()\n",
    "\n",
    "    n_orig = count_params(orig)\n",
    "    n_merged_full = count_params(merged_full)\n",
    "    n_compressed = count_params(compressed)\n",
    "\n",
    "    print(f\"Params - original:   {n_orig:,}\")\n",
    "    print(f\"Params - merged_full (same depth): {n_merged_full:,}  (should match original; merging alone doesnâ€™t cut params)\")\n",
    "    print(f\"Params - compressed_half (half depth): {n_compressed:,}  (reduced)\")\n",
    "\n",
    "    # quick sanity generation\n",
    "    prompt = \"The future of artificial intelligence in India\"\n",
    "    inp = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_orig = orig.generate(**inp, max_length=40, do_sample=False)\n",
    "        out_comp = compressed.generate(**inp, max_length=40, do_sample=False)\n",
    "\n",
    "    print(\"\\nOriginal:\\n\", tok.decode(out_orig[0], skip_special_tokens=True))\n",
    "    print(\"\\nCompressed (half layers):\\n\", tok.decode(out_comp[0], skip_special_tokens=True))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "956f0b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2 Perplexity: 851.85\n",
      "Compressed GPT-2 (k-merged) Perplexity: 23989.89\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "texts = [t for t in dataset['text'] if len(t.strip()) > 0][:100]  # use 100 lines for speed\n",
    "\n",
    "def compute_perplexity(model_name, texts, batch_size=4):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name).to(device).eval()\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
    "\n",
    "        num_tokens = inputs[\"input_ids\"].numel()\n",
    "        total_loss += loss.item() * num_tokens\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return ppl\n",
    "\n",
    "orig_ppl = compute_perplexity(\"gpt2\", texts)\n",
    "compressed_ppl = compute_perplexity(\"gpt2_compressed_half\", texts)\n",
    "\n",
    "print(f\"Original GPT-2 Perplexity: {orig_ppl:.2f}\")\n",
    "print(f\"Compressed GPT-2 (k-merged) Perplexity: {compressed_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f76c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9cf89b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training original GPT-2...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36718' max='36718' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36718/36718 46:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.368800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.304900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>1.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>1.085800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>1.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>1.106500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training compressed GPT-2...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36718' max='36718' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36718/36718 30:27, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.788100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.670100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.627900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.581100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.465800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>1.441700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.421800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>1.403800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>1.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>1.427700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 8.50kB [00:00, 6.57MB/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m     results \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, add_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, texts\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m200\u001b[39m])\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m orig_ppl \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m compressed_ppl \u001b[38;5;241m=\u001b[39m compute_perplexity(compressed_model, dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplexity (Original GPT-2 after training): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig_ppl\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[43], line 89\u001b[0m, in \u001b[0;36mcompute_perplexity\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     87\u001b[0m metric \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/myexamenv/lib/python3.9/site-packages/evaluate/module.py:456\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilelock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myexamenv/lib/python3.9/site-packages/evaluate/module.py:399\u001b[0m, in \u001b[0;36mEvaluationModule._finalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_buffer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Let's acquire a lock on each node files to be sure they are finished writing\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     file_paths, filelocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_all_cache_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Read the predictions and references\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/myexamenv/lib/python3.9/site-packages/evaluate/module.py:316\u001b[0m, in \u001b[0;36mEvaluationModule._get_all_cache_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_process \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    317\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation module cache file doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist. Please make sure that you call `add` or `add_batch` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat least once before calling `compute`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m         )\n\u001b[1;32m    320\u001b[0m     file_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import math\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --------------------------\n",
    "# 1. Load Models\n",
    "# --------------------------\n",
    "orig_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "compressed_model = GPT2LMHeadModel.from_pretrained(\"gpt2_compressed_half\").to(device)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --------------------------\n",
    "# 2. Load WikiText-2 Dataset\n",
    "# --------------------------\n",
    "# --------------------------\n",
    "# 2. Load + preprocess WikiText-2 properly\n",
    "# --------------------------\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    out = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=128\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized.set_format(type=\"torch\")\n",
    "train_data = tokenized[\"train\"]\n",
    "test_data = tokenized[\"test\"]\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3. Training Function\n",
    "# --------------------------\n",
    "def train_model(model, save_dir):\n",
    "    # Make sure gradients are enabled and model is in training mode\n",
    "    torch.set_grad_enabled(True)\n",
    "    model.train()\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        eval_strategy=\"no\",\n",
    "        per_device_train_batch_size=2,\n",
    "        num_train_epochs=2,\n",
    "        logging_steps=2000,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        report_to=\"none\",\n",
    "        \n",
    "        # â†“ Add these lines â†“\n",
    "        save_strategy=\"steps\",    # or \"epoch\" or \"no\"\n",
    "        save_steps=20000,         # save every 50k steps\n",
    "        save_total_limit=1,       # keep only the last checkpoint\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=train_data)\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(save_dir)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4. Train both models\n",
    "# --------------------------\n",
    "print(\"Training original GPT-2...\")\n",
    "train_model(orig_model, \"gpt2_finetuned_orig\")\n",
    "\n",
    "print(\"Training compressed GPT-2...\")\n",
    "train_model(compressed_model, \"gpt2_finetuned_compressed\")\n",
    "\n",
    "# --------------------------\n",
    "# 5. Perplexity Evaluation\n",
    "# --------------------------\n",
    "def compute_perplexity(model, data):\n",
    "    metric = evaluate.load(\"perplexity\")\n",
    "    model.eval()\n",
    "    results = metric.compute(model_id=None, model=model, tokenizer=tokenizer, add_start_token=False, texts=data[\"text\"][:200])\n",
    "    return results[\"perplexity\"]\n",
    "\n",
    "orig_ppl = compute_perplexity(orig_model, dataset[\"test\"])\n",
    "compressed_ppl = compute_perplexity(compressed_model, dataset[\"test\"])\n",
    "\n",
    "print(f\"Perplexity (Original GPT-2 after training): {orig_ppl:.2f}\")\n",
    "print(f\"Perplexity (Compressed GPT-2 after training): {compressed_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e745c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_perplexity(model, data, tokenizer, max_samples=200, batch_size=2):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Take up to `max_samples` non-empty text samples\n",
    "    texts = [t for t in data[\"text\"] if isinstance(t, str) and len(t.strip()) > 0][:max_samples]\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing perplexity\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        encodings = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encodings, labels=encodings[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            # Multiply by number of tokens to get total weighted loss\n",
    "            total_loss += loss.item() * encodings[\"input_ids\"].numel()\n",
    "            total_tokens += encodings[\"input_ids\"].numel()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15b629e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing perplexity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 60.46it/s]\n",
      "Computing perplexity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 105.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (Original GPT-2 after training): 12.28\n",
      "Perplexity (Compressed GPT-2 after training): 18.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "orig_ppl = compute_perplexity(orig_model, dataset[\"test\"], tokenizer)\n",
    "compressed_ppl = compute_perplexity(compressed_model, dataset[\"test\"], tokenizer)\n",
    "\n",
    "print(f\"Perplexity (Original GPT-2 after training): {orig_ppl:.2f}\")\n",
    "print(f\"Perplexity (Compressed GPT-2 after training): {compressed_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "682148ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2 total params: 124,439,808\n",
      "Original GPT-2 trainable params: 124,439,808\n",
      "Compressed GPT-2 total params: 81,912,576\n",
      "Compressed GPT-2 trainable params: 81,912,576\n",
      "Parameter reduction: 34.17%\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "orig_total, orig_trainable = count_parameters(orig_model)\n",
    "comp_total, comp_trainable = count_parameters(compressed_model)\n",
    "\n",
    "print(f\"Original GPT-2 total params: {orig_total:,}\")\n",
    "print(f\"Original GPT-2 trainable params: {orig_trainable:,}\")\n",
    "print(f\"Compressed GPT-2 total params: {comp_total:,}\")\n",
    "print(f\"Compressed GPT-2 trainable params: {comp_trainable:,}\")\n",
    "\n",
    "reduction = 100 * (1 - comp_total / orig_total)\n",
    "print(f\"Parameter reduction: {reduction:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f962645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency (Original GPT-2): 14.27 ms/sample\n",
      "Average latency (Compressed GPT-2): 127.10 ms/sample\n",
      "Speedup: 0.11x faster\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def measure_latency(model, tokenizer, device=\"cuda\", num_samples=20, max_length=64):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create random sample inputs (or you can use your dataset)\n",
    "    texts = [\"The quick brown fox jumps over the lazy dog.\"] * num_samples\n",
    "\n",
    "    # Warm-up (to avoid first-run overhead)\n",
    "    for _ in range(3):\n",
    "        _ = model.generate(\n",
    "            **tokenizer(texts[0], return_tensors=\"pt\").to(device),\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "\n",
    "    start_time = time.time()\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_length=max_length)\n",
    "    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    avg_latency = total_time / num_samples\n",
    "    return avg_latency\n",
    "\n",
    "orig_latency = measure_latency(orig_model, tokenizer, device=device)\n",
    "comp_latency = measure_latency(compressed_model, tokenizer, device=device)\n",
    "\n",
    "print(f\"Average latency (Original GPT-2): {orig_latency*1000:.2f} ms/sample\")\n",
    "print(f\"Average latency (Compressed GPT-2): {comp_latency*1000:.2f} ms/sample\")\n",
    "\n",
    "speedup = orig_latency / comp_latency\n",
    "print(f\"Speedup: {speedup:.2f}x faster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fdff31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Evaluating ROUGE for: original_gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 1/100 [00:00<00:40,  2.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  2%|â–         | 2/100 [00:00<00:35,  2.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  3%|â–Ž         | 3/100 [00:01<00:32,  2.96it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|â–         | 4/100 [00:01<00:31,  3.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  5%|â–Œ         | 5/100 [00:01<00:30,  3.10it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  6%|â–Œ         | 6/100 [00:01<00:30,  3.13it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  7%|â–‹         | 7/100 [00:02<00:29,  3.14it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|â–Š         | 8/100 [00:02<00:23,  3.99it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  9%|â–‰         | 9/100 [00:02<00:24,  3.71it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 10%|â–ˆ         | 10/100 [00:03<00:25,  3.55it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 11%|â–ˆ         | 11/100 [00:03<00:25,  3.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|â–ˆâ–        | 12/100 [00:03<00:26,  3.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 13%|â–ˆâ–Ž        | 13/100 [00:03<00:26,  3.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 14%|â–ˆâ–        | 14/100 [00:04<00:26,  3.26it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 15%|â–ˆâ–Œ        | 15/100 [00:04<00:26,  3.25it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 16%|â–ˆâ–Œ        | 16/100 [00:04<00:26,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|â–ˆâ–‹        | 17/100 [00:05<00:25,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 18%|â–ˆâ–Š        | 18/100 [00:05<00:25,  3.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 19%|â–ˆâ–‰        | 19/100 [00:05<00:25,  3.19it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|â–ˆâ–ˆ        | 20/100 [00:06<00:24,  3.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|â–ˆâ–ˆ        | 21/100 [00:06<00:24,  3.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 22%|â–ˆâ–ˆâ–       | 22/100 [00:06<00:24,  3.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:07<00:24,  3.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 24%|â–ˆâ–ˆâ–       | 24/100 [00:07<00:23,  3.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:07<00:23,  3.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:08<00:23,  3.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 27%|â–ˆâ–ˆâ–‹       | 27/100 [00:08<00:22,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 28%|â–ˆâ–ˆâ–Š       | 28/100 [00:08<00:22,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|â–ˆâ–ˆâ–‰       | 29/100 [00:08<00:22,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:09<00:21,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:09<00:21,  3.23it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:09<00:21,  3.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:10<00:20,  3.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:10<00:19,  3.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:10<00:19,  3.31it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:11<00:19,  3.27it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:11<00:19,  3.24it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:11<00:19,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:12<00:18,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:12<00:18,  3.19it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:12<00:18,  3.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:12<00:18,  3.18it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:13<00:17,  3.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:13<00:17,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:13<00:17,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:14<00:16,  3.23it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:14<00:14,  3.66it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:14<00:14,  3.48it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:15<00:15,  3.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:15<00:15,  3.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:15<00:15,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:16<00:14,  3.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:16<00:14,  3.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:16<00:14,  3.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:16<00:13,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:17<00:13,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:17<00:12,  3.44it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:17<00:12,  3.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:18<00:10,  3.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:18<00:11,  3.56it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:18<00:11,  3.46it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:18<00:11,  3.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:19<00:11,  3.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:19<00:10,  3.30it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:19<00:10,  3.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:20<00:10,  3.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:20<00:10,  3.26it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:20<00:09,  3.26it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:21<00:09,  3.27it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:21<00:09,  3.26it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:21<00:08,  3.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:21<00:08,  3.31it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:22<00:08,  3.27it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:22<00:08,  3.24it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:22<00:07,  3.24it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:23<00:07,  3.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:23<00:07,  3.23it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:23<00:06,  3.23it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:24<00:06,  3.23it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:24<00:06,  3.24it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:24<00:05,  3.23it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:25<00:05,  3.24it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:25<00:04,  3.90it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:25<00:03,  4.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:26<00:03,  4.02it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:26<00:03,  3.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:26<00:03,  3.66it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:27<00:02,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:27<00:02,  4.27it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:27<00:02,  3.88it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:27<00:01,  3.67it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:28<00:01,  3.53it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:28<00:01,  3.44it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:28<00:01,  3.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:29<00:00,  3.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [00:29<00:00,  3.91it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:29<00:00,  3.66it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:29<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ROUGE scores for original_gpt2:\n",
      "{'rouge1': np.float64(0.23686015230383223), 'rouge2': np.float64(0.10447509226414367), 'rougeL': np.float64(0.1703181031849859), 'rougeLsum': np.float64(0.19939702492949057)}\n",
      "\n",
      "ðŸ”¹ Evaluating ROUGE for: compressed_gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 1/100 [00:00<00:21,  4.50it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  2%|â–         | 2/100 [00:00<00:21,  4.63it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  3%|â–Ž         | 3/100 [00:00<00:20,  4.67it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|â–         | 4/100 [00:00<00:20,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  5%|â–Œ         | 5/100 [00:01<00:20,  4.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  6%|â–Œ         | 6/100 [00:01<00:19,  4.74it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  7%|â–‹         | 7/100 [00:01<00:19,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|â–Š         | 8/100 [00:01<00:19,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  9%|â–‰         | 9/100 [00:01<00:19,  4.71it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 10%|â–ˆ         | 10/100 [00:02<00:19,  4.71it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 11%|â–ˆ         | 11/100 [00:02<00:18,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|â–ˆâ–        | 12/100 [00:02<00:18,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 13%|â–ˆâ–Ž        | 13/100 [00:02<00:18,  4.74it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 14%|â–ˆâ–        | 14/100 [00:02<00:18,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 15%|â–ˆâ–Œ        | 15/100 [00:03<00:17,  4.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 16%|â–ˆâ–Œ        | 16/100 [00:03<00:17,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|â–ˆâ–‹        | 17/100 [00:03<00:17,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 18%|â–ˆâ–Š        | 18/100 [00:03<00:17,  4.69it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 19%|â–ˆâ–‰        | 19/100 [00:04<00:17,  4.69it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|â–ˆâ–ˆ        | 20/100 [00:04<00:17,  4.68it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|â–ˆâ–ˆ        | 21/100 [00:04<00:16,  4.68it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 22%|â–ˆâ–ˆâ–       | 22/100 [00:04<00:16,  4.69it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:04<00:16,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 24%|â–ˆâ–ˆâ–       | 24/100 [00:05<00:16,  4.71it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:05<00:15,  4.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:05<00:15,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 27%|â–ˆâ–ˆâ–‹       | 27/100 [00:05<00:15,  4.71it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 28%|â–ˆâ–ˆâ–Š       | 28/100 [00:05<00:15,  4.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|â–ˆâ–ˆâ–‰       | 29/100 [00:06<00:15,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:06<00:14,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:06<00:14,  4.74it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:14,  4.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:06<00:14,  4.76it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:13,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:07<00:13,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:07<00:13,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:07<00:13,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:08<00:12,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:08<00:12,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:08<00:12,  4.76it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:08<00:12,  4.78it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:08<00:12,  4.76it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:09<00:11,  4.78it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:09<00:11,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:09<00:11,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:09<00:11,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:09<00:11,  4.76it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:10<00:11,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:10<00:10,  4.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:10<00:10,  4.76it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:10<00:10,  4.76it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:10<00:10,  4.77it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:11<00:09,  4.78it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:11<00:09,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:11<00:09,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:11<00:09,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:12<00:08,  4.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:12<00:08,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:12<00:08,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:12<00:08,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:12<00:08,  4.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:13<00:07,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:13<00:07,  4.83it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:13<00:07,  4.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:13<00:07,  4.83it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:13<00:07,  4.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:14<00:06,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:14<00:06,  4.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:14<00:06,  4.75it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:14<00:06,  4.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:14<00:06,  4.76it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:15<00:05,  4.78it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:15<00:05,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:15<00:05,  4.78it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:15<00:05,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:15<00:05,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:16<00:04,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:16<00:04,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:16<00:04,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:16<00:04,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:17<00:03,  4.77it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:17<00:03,  4.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:17<00:03,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:17<00:03,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:17<00:03,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:18<00:02,  4.76it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:18<00:02,  4.78it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:18<00:02,  4.87it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:18<00:02,  4.84it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:18<00:02,  4.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:19<00:01,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:19<00:01,  4.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:19<00:01,  4.69it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:19<00:01,  4.70it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:19<00:01,  4.72it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:20<00:00,  4.71it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:20<00:00,  4.75it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [00:20<00:00,  4.74it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:20<00:00,  4.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:21<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ROUGE scores for compressed_gpt2:\n",
      "{'rouge1': np.float64(0.20007001564974664), 'rouge2': np.float64(0.08813575833280402), 'rougeL': np.float64(0.14050997503585588), 'rougeLsum': np.float64(0.16410708368153099)}\n",
      "\n",
      "ðŸ“Š Final ROUGE Comparison:\n",
      "original_gpt2 â†’ ROUGE-L: 0.1703181031849859\n",
      "compressed_gpt2 â†’ ROUGE-L: 0.14050997503585588\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load dataset (CNN/DailyMail test split)\n",
    "data = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:100]\")  # only first 100 for speed\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Load tokenizer\n",
    "tok = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "# Models to test\n",
    "models = {\n",
    "    \"original_gpt2\": \"gpt2\",\n",
    "    \"compressed_gpt2\": \"gpt2_compressed_half\"\n",
    "}\n",
    "\n",
    "def generate_summary(model, text, max_new_tokens=60):\n",
    "    inp = tok(\n",
    "        text, return_tensors=\"pt\",\n",
    "        truncation=True, padding=True, max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=max_new_tokens,     # âœ… instead of max_length\n",
    "        num_beams=3,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "    return tok.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, path in models.items():\n",
    "    print(f\"\\nðŸ”¹ Evaluating ROUGE for: {name}\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(path).to(device).eval()\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        article = data[i][\"article\"]\n",
    "        ref = data[i][\"highlights\"]\n",
    "\n",
    "        pred = generate_summary(model, article)\n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "\n",
    "    score = rouge.compute(predictions=predictions, references=references)\n",
    "    results[name] = score\n",
    "    print(f\"âœ… ROUGE scores for {name}:\")\n",
    "    print(score)\n",
    "\n",
    "print(\"\\nðŸ“Š Final ROUGE Comparison:\")\n",
    "for model_name, score in results.items():\n",
    "    print(model_name, \"â†’ ROUGE-L:\", score[\"rougeL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146d4a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Training Merged GPT-2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7424/4199903884.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25000/25000 24:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.471400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.745700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.713200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.714900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.788200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.897400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.771100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.771200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.716600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.767100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.701800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.688600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.727400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.679900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.863200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.626300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.780300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.744500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.893300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.814400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.979800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.834800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.934200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.792200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.778300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.982800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.835800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.760700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.893300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.946200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.889400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.766200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.954200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.833100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.845700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.824700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.626200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.932300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.984700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.821500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.869400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.935700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.948800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.812700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.735800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.914200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.664300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.935200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.869600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.762600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.903200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.788200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.903800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.861300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.918600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>1.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.091900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.845500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.644900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.579700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>1.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.746200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>1.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.913300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>0.826900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.555600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>0.864900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.795100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>0.847200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>0.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>0.694600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>0.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.976700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>0.819500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>0.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.955500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>0.801500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.886100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.118400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>0.853300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.660400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>0.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>1.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>1.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.752600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>0.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>1.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>0.842300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>1.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>1.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.721200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>1.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.626800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>0.875400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.897800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>1.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>0.646500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>1.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>0.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>1.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.932900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>0.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>0.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>0.901100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.452100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>1.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.799200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>0.749700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.779500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>0.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.581200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>1.091700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.791200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>0.760400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.239100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>0.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.815900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>0.858500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.777500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>0.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>1.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.724400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>1.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>0.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.711100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>0.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.675800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>0.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>0.923200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>1.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>1.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.741100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>0.679500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.943300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>0.827400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>0.754200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.617600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>0.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.871900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>0.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>0.774500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.730100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>0.542900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>0.796800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12850</td>\n",
       "      <td>0.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12950</td>\n",
       "      <td>0.606200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.638400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13050</td>\n",
       "      <td>0.647800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.903500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13150</td>\n",
       "      <td>1.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.560800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>0.874600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13350</td>\n",
       "      <td>0.694800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13450</td>\n",
       "      <td>0.740100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13550</td>\n",
       "      <td>0.718300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.616300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13650</td>\n",
       "      <td>0.875400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.463800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>1.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13850</td>\n",
       "      <td>0.717200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.675800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13950</td>\n",
       "      <td>0.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.758400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14050</td>\n",
       "      <td>0.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14150</td>\n",
       "      <td>0.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>0.669500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14350</td>\n",
       "      <td>0.721200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>0.890800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14550</td>\n",
       "      <td>0.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.762400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14650</td>\n",
       "      <td>1.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.497900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>0.645300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14850</td>\n",
       "      <td>0.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14950</td>\n",
       "      <td>0.742600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15050</td>\n",
       "      <td>0.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15150</td>\n",
       "      <td>0.818100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>0.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.794200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15350</td>\n",
       "      <td>0.698400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15450</td>\n",
       "      <td>1.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15550</td>\n",
       "      <td>0.813100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15650</td>\n",
       "      <td>0.551200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15750</td>\n",
       "      <td>0.534700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.607100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15850</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.588700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15950</td>\n",
       "      <td>0.931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.453100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16050</td>\n",
       "      <td>0.782300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16150</td>\n",
       "      <td>0.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>0.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16350</td>\n",
       "      <td>0.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.849100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16450</td>\n",
       "      <td>0.227900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16550</td>\n",
       "      <td>0.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.735000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16650</td>\n",
       "      <td>0.769500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.879300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16750</td>\n",
       "      <td>0.579400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16850</td>\n",
       "      <td>0.810700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.801500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16950</td>\n",
       "      <td>0.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.576400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17050</td>\n",
       "      <td>0.675200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.741900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17150</td>\n",
       "      <td>0.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17250</td>\n",
       "      <td>0.639300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.677900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17350</td>\n",
       "      <td>0.676600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17450</td>\n",
       "      <td>0.767700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.551400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17550</td>\n",
       "      <td>0.718100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.643200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17650</td>\n",
       "      <td>0.364300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.780100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17750</td>\n",
       "      <td>0.564800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.596200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17850</td>\n",
       "      <td>0.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17950</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18050</td>\n",
       "      <td>0.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18150</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.770300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18250</td>\n",
       "      <td>0.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.869400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18350</td>\n",
       "      <td>0.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18450</td>\n",
       "      <td>0.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18550</td>\n",
       "      <td>0.788400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18650</td>\n",
       "      <td>0.839500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18750</td>\n",
       "      <td>0.553200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.428700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18850</td>\n",
       "      <td>0.499200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.847200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18950</td>\n",
       "      <td>0.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19050</td>\n",
       "      <td>0.536400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.743300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19150</td>\n",
       "      <td>0.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19250</td>\n",
       "      <td>0.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19350</td>\n",
       "      <td>0.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.965800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19450</td>\n",
       "      <td>0.438400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.660100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19550</td>\n",
       "      <td>0.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.496100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19650</td>\n",
       "      <td>0.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.697600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19750</td>\n",
       "      <td>0.384200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19850</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19950</td>\n",
       "      <td>0.748200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20050</td>\n",
       "      <td>0.617200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20150</td>\n",
       "      <td>0.671600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.575100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20250</td>\n",
       "      <td>0.979500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20350</td>\n",
       "      <td>0.853200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.623200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20450</td>\n",
       "      <td>0.622400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.653300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20550</td>\n",
       "      <td>0.775900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.760100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20650</td>\n",
       "      <td>0.644400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.645900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20750</td>\n",
       "      <td>0.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20850</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.554500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20950</td>\n",
       "      <td>0.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.344300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21050</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.701200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21150</td>\n",
       "      <td>0.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21250</td>\n",
       "      <td>0.550900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.567300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21350</td>\n",
       "      <td>0.727900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.658100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21450</td>\n",
       "      <td>0.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.606700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21550</td>\n",
       "      <td>0.810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.774500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21650</td>\n",
       "      <td>0.851200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.719800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21750</td>\n",
       "      <td>0.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.514800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21850</td>\n",
       "      <td>0.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.977500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21950</td>\n",
       "      <td>0.593000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22050</td>\n",
       "      <td>0.621500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22150</td>\n",
       "      <td>0.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.927700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22250</td>\n",
       "      <td>0.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22350</td>\n",
       "      <td>0.322300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22450</td>\n",
       "      <td>0.857100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.385300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22550</td>\n",
       "      <td>0.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22650</td>\n",
       "      <td>0.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.458400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22750</td>\n",
       "      <td>0.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.543700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22850</td>\n",
       "      <td>1.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>0.572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22950</td>\n",
       "      <td>0.727200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.616300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23050</td>\n",
       "      <td>0.498400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23150</td>\n",
       "      <td>0.600500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.675900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23250</td>\n",
       "      <td>0.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>0.628500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23350</td>\n",
       "      <td>0.516600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23450</td>\n",
       "      <td>0.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23550</td>\n",
       "      <td>0.766200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23650</td>\n",
       "      <td>0.774800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>0.709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23750</td>\n",
       "      <td>0.668900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.772800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23850</td>\n",
       "      <td>0.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>0.581400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23950</td>\n",
       "      <td>0.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24050</td>\n",
       "      <td>0.606400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>0.747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24150</td>\n",
       "      <td>0.699400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>0.850700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24250</td>\n",
       "      <td>0.666600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24350</td>\n",
       "      <td>0.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>1.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24450</td>\n",
       "      <td>0.505500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24550</td>\n",
       "      <td>0.638900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24650</td>\n",
       "      <td>0.406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>0.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24750</td>\n",
       "      <td>0.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24850</td>\n",
       "      <td>0.837600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>0.859900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24950</td>\n",
       "      <td>0.815500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.646500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged GPT-2 Test Accuracy: 83.67%\n",
      "\n",
      "ðŸ”¹ Training Compressed GPT-2 ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25000/25000 15:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.354500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.773100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.768500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.627300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.852400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.679300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.737700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.676400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.935100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.963400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.834100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.849100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.791900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.986500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.751700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.670700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.759900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.714700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.765200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.703400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.796200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.807300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.779600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.868300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.969800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.653900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.807200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.743700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.900900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.961700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.561800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.846000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.926600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.900900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.788200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.848700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.650900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.937300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.792500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.829200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.914900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.944100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.914900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.628600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.934700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.778000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.787200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.792500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.962100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.846100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.840600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.971800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.799600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.808300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.659600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.803900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.904900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.595300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.998700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.771300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.872600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.752800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.885200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.880700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.726700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.804800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.483800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.993600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.913900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.675100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.858900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.583600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>0.714700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.617200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>0.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.931200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>0.812800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>0.860900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>0.774100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>0.871900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.831800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.937300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.549300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>0.744700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.700800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>0.577800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.780300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>0.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.788100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>0.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.617500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.900200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>0.857000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>0.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>0.919200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.757500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>0.943500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.555400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.501400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.832400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>0.766600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>1.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.834700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>0.469500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>0.738200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>0.906100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.759600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>1.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.604900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>0.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.541200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>0.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.932900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>0.630300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.876300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>0.722300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>0.772800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.595300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>1.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>0.676500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>0.515200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>0.765300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.534900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>0.947500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.604600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>0.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>0.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>0.821500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.547800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>0.729100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>0.674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.799200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>0.815400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>0.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.792600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>0.813500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.795900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>1.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.743700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>0.632100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>0.773700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>1.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>0.748400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>0.932700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.547600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>0.880400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>0.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>0.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.904900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>0.520500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>0.778700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.993100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>0.625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.823400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>0.652800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.482700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.769300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>0.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.544500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>0.775900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.832700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12850</td>\n",
       "      <td>0.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12950</td>\n",
       "      <td>0.612700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.571800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13050</td>\n",
       "      <td>0.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13150</td>\n",
       "      <td>1.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.420300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>0.676600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.830900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13350</td>\n",
       "      <td>0.736900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.668200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13450</td>\n",
       "      <td>0.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13550</td>\n",
       "      <td>0.776500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13650</td>\n",
       "      <td>0.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>0.887300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.398900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13850</td>\n",
       "      <td>0.706700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13950</td>\n",
       "      <td>0.347700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14050</td>\n",
       "      <td>0.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14150</td>\n",
       "      <td>0.311600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>0.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.562200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14350</td>\n",
       "      <td>0.736900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>0.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14550</td>\n",
       "      <td>0.609400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.625400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14650</td>\n",
       "      <td>0.841600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.224400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>0.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14850</td>\n",
       "      <td>0.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14950</td>\n",
       "      <td>0.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15050</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.714100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15150</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>0.543600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.778100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15350</td>\n",
       "      <td>0.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15450</td>\n",
       "      <td>1.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15550</td>\n",
       "      <td>0.754200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15650</td>\n",
       "      <td>0.566800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.591300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15750</td>\n",
       "      <td>0.524100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15850</td>\n",
       "      <td>0.576300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.466500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15950</td>\n",
       "      <td>1.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16050</td>\n",
       "      <td>0.578100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.602100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16150</td>\n",
       "      <td>0.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.717300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>0.641200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.819400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16350</td>\n",
       "      <td>0.670800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.654700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16450</td>\n",
       "      <td>0.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16550</td>\n",
       "      <td>0.719300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16650</td>\n",
       "      <td>0.726900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16750</td>\n",
       "      <td>0.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.354800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16850</td>\n",
       "      <td>0.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16950</td>\n",
       "      <td>0.708300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17050</td>\n",
       "      <td>0.587500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.573800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17150</td>\n",
       "      <td>0.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.560500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17250</td>\n",
       "      <td>0.626100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17350</td>\n",
       "      <td>0.707100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.441700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17450</td>\n",
       "      <td>0.810200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.487500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17550</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17650</td>\n",
       "      <td>0.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.541400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17750</td>\n",
       "      <td>0.573400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.713500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17850</td>\n",
       "      <td>0.420600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17950</td>\n",
       "      <td>0.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.580200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18050</td>\n",
       "      <td>0.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18150</td>\n",
       "      <td>0.617600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.725200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18250</td>\n",
       "      <td>0.574800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.782200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18350</td>\n",
       "      <td>0.429900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.659100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18450</td>\n",
       "      <td>0.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18550</td>\n",
       "      <td>0.731100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.738900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18650</td>\n",
       "      <td>0.636300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.635100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18750</td>\n",
       "      <td>0.494200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.576900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18850</td>\n",
       "      <td>0.641400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.831400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18950</td>\n",
       "      <td>0.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.768500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19050</td>\n",
       "      <td>0.465700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.659900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19150</td>\n",
       "      <td>0.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.520900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19250</td>\n",
       "      <td>0.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19350</td>\n",
       "      <td>0.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.834600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19450</td>\n",
       "      <td>0.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19550</td>\n",
       "      <td>0.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.472200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19650</td>\n",
       "      <td>0.854200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19750</td>\n",
       "      <td>0.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.257800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19850</td>\n",
       "      <td>0.397900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.948500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19950</td>\n",
       "      <td>0.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20050</td>\n",
       "      <td>0.616700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20150</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.789900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20250</td>\n",
       "      <td>0.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20350</td>\n",
       "      <td>0.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20450</td>\n",
       "      <td>0.550900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.604500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20550</td>\n",
       "      <td>0.702700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.659500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20650</td>\n",
       "      <td>0.480900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.662800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20750</td>\n",
       "      <td>0.533700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.794600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20850</td>\n",
       "      <td>0.610900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20950</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21050</td>\n",
       "      <td>0.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.772300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21150</td>\n",
       "      <td>0.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.799500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21250</td>\n",
       "      <td>0.550300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21350</td>\n",
       "      <td>0.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21450</td>\n",
       "      <td>0.557400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.486000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21550</td>\n",
       "      <td>0.591200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21650</td>\n",
       "      <td>0.873500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21750</td>\n",
       "      <td>0.475400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.442400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21850</td>\n",
       "      <td>0.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.736900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21950</td>\n",
       "      <td>0.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.912900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22050</td>\n",
       "      <td>0.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22150</td>\n",
       "      <td>0.525600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.783900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22250</td>\n",
       "      <td>0.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.431100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22350</td>\n",
       "      <td>0.614600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22450</td>\n",
       "      <td>0.684300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22550</td>\n",
       "      <td>0.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22650</td>\n",
       "      <td>0.534100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22750</td>\n",
       "      <td>0.533200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22850</td>\n",
       "      <td>0.840500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>0.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22950</td>\n",
       "      <td>0.771200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23050</td>\n",
       "      <td>0.484600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23150</td>\n",
       "      <td>0.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23250</td>\n",
       "      <td>0.716800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>0.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23350</td>\n",
       "      <td>0.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23450</td>\n",
       "      <td>0.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23550</td>\n",
       "      <td>0.569300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23650</td>\n",
       "      <td>0.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>0.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23750</td>\n",
       "      <td>0.503400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23850</td>\n",
       "      <td>0.571900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>0.475500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23950</td>\n",
       "      <td>0.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24050</td>\n",
       "      <td>0.589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>0.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24150</td>\n",
       "      <td>0.633400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>0.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24250</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24350</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.815100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24450</td>\n",
       "      <td>0.586700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24550</td>\n",
       "      <td>0.798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.752100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24650</td>\n",
       "      <td>0.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>0.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24750</td>\n",
       "      <td>0.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.572500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24850</td>\n",
       "      <td>0.787400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>0.915100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24950</td>\n",
       "      <td>0.837800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.439100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Compressed GPT-2 Test Accuracy: 83.58%\n",
      "\n",
      "ðŸ“Š Final IMDb Accuracies:\n",
      "  Merged GPT-2:     83.67%\n",
      "  Compressed GPT-2: 83.58%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------------\n",
    "# 1ï¸âƒ£ Load tokenizer\n",
    "# -------------------------------\n",
    "tok = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "# -------------------------------\n",
    "# 2ï¸âƒ£ Prepare IMDB dataset\n",
    "# -------------------------------\n",
    "ds = load_dataset(\"imdb\")\n",
    "def tok_fn(e):\n",
    "    return tok(e[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "ds = ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "# -------------------------------\n",
    "# 3ï¸âƒ£ Sentiment classification wrapper\n",
    "# -------------------------------\n",
    "class GPT2ForSentiment(nn.Module):\n",
    "    def __init__(self, gpt_model):\n",
    "        super().__init__()\n",
    "        self.gpt2 = gpt_model\n",
    "        self.classifier = nn.Linear(self.gpt2.config.n_embd, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        # âœ… fix: use final non-padding token, not last fixed position\n",
    "        lengths = attention_mask.sum(dim=1) - 1\n",
    "        last_hidden = outputs.hidden_states[-1][torch.arange(input_ids.size(0)), lengths, :]\n",
    "        logits = self.classifier(last_hidden)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.functional.cross_entropy(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# -------------------------------\n",
    "# 4ï¸âƒ£ Training setup (same as pruning config)\n",
    "# -------------------------------\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_imdb_results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 5ï¸âƒ£ Train & Evaluate function\n",
    "# -------------------------------\n",
    "def train_and_eval(model, name):\n",
    "    print(f\"\\nðŸ”¹ Training {name} ...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"test\"],\n",
    "        tokenizer=tok,\n",
    "        data_collator=collator\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # ---- Evaluate ----\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for batch in torch.utils.data.DataLoader(ds[\"test\"], batch_size=8):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        preds = out[\"logits\"].argmax(dim=-1)\n",
    "        correct += (preds == batch[\"label\"]).sum().item()\n",
    "        total += batch[\"label\"].size(0)\n",
    "    acc = correct / total\n",
    "    print(f\"âœ… {name} Test Accuracy: {acc*100:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "# -------------------------------\n",
    "# 6ï¸âƒ£ Evaluate both models\n",
    "# -------------------------------\n",
    "# ---- Merged GPT-2 ----\n",
    "m_merged = GPT2LMHeadModel.from_pretrained(\"gpt2_merged_full\")\n",
    "model_merged = GPT2ForSentiment(m_merged.to(device))\n",
    "acc_merged = train_and_eval(model_merged, \"Merged GPT-2\")\n",
    "\n",
    "# ---- Compressed GPT-2 ----\n",
    "m_compressed = GPT2LMHeadModel.from_pretrained(\"gpt2_compressed_half\")\n",
    "model_compressed = GPT2ForSentiment(m_compressed.to(device))\n",
    "acc_compressed = train_and_eval(model_compressed, \"Comp ressed GPT-2\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7ï¸âƒ£ Summary\n",
    "# -------------------------------\n",
    "print(\"\\nðŸ“Š Final IMDb Accuracies:\")\n",
    "print(f\"  Merged GPT-2:     {acc_merged*100:.2f}%\")\n",
    "print(f\"  Compressed GPT-2: {acc_compressed*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130c234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myexamenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
